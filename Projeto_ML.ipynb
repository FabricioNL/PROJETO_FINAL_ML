{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto de Machine Learning | Processamento Natural de Linguagem\n",
    "\n",
    "> Jean Sanandrez | Fabricio Neri | Luiza Valezim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O primeiro passo que precisaremos fazer é importar as principais bibliotecas que serão utilizadas no processo. O tensorflow e o sklearn são extremamente importantes nos processos de aplicação de ML no projeto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "01a42a6e"
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow_text --quiet\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as text  # Registers the ops.\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4ecce59"
   },
   "source": [
    "# Leitura dos dados\n",
    "\n",
    "Os dados são originários de uma competição do Kaggle, em que você precisa criar um modelo que relacione uma âncora, um contexto e uma frase e retorne em um score de 0 a 1.0 o quanto a frase e a âncora estão relacionados através do mesmo contexto. \n",
    "\n",
    "O link da competição pode ser acessado a seguir: [clique aqui para ser direcionado ](https://www.kaggle.com/competitions/us-patent-phrase-to-phrase-matching/data?select=train.csv)\n",
    "\n",
    "Os dados dados de treino originários possuem um total de 37 mil linhas, contudo, se tornava inviável processar todos esses dados pois o tempo de processamento ultrapassava 5h para cada ação, então fizemos uma amostragem dos dados.\n",
    "\n",
    "Mantivemos apenas as linhas que possuíam contextos com frequências mais significativas. Dois contextos em específicos possuiam somados 70% da frequência dos dados, então mantivemos os dois.\n",
    "\n",
    "Caso queira verificar o funcionamento do projeto com os dados totais, iremos enviar também os dados originais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "2e0f5b9c"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "train = pd.read_csv(\"train_filter.csv\")\n",
    "test = pd.read_csv(\"test_filter.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K5q8cbttQlh9"
   },
   "source": [
    "Abaixo é possível verificar as principais colunas da tabela de treino. Apenas a coluna de ID não possui valor para um futuro modelo. Além disso, não é possível realizar uma análise exploratória propriamente dita porque por se tratar de um projeto de NLP, e em específico o batimento de âncora e target por um contexto, frequência de palavras em frases e etc, métricas importantes em NLP, não fazem tanto sentido neste caso, porque tudo depende do contexto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "29928f59",
    "outputId": "ae323228-7d83-4610-b84a-08efae2031a6",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>anchor</th>\n",
       "      <th>target</th>\n",
       "      <th>context</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6bdd1d05ffa3401e</td>\n",
       "      <td>abatement</td>\n",
       "      <td>emission abatement</td>\n",
       "      <td>H04</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a937ae580ea4bd97</td>\n",
       "      <td>achieve authentication</td>\n",
       "      <td>achieve access</td>\n",
       "      <td>H04</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ea154b802bebc48a</td>\n",
       "      <td>achieve authentication</td>\n",
       "      <td>achieve authentication accuracy</td>\n",
       "      <td>H04</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24eda11bf0665917</td>\n",
       "      <td>achieve authentication</td>\n",
       "      <td>achieve authorization</td>\n",
       "      <td>H04</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14c027729175aedd</td>\n",
       "      <td>achieve authentication</td>\n",
       "      <td>achieve goal</td>\n",
       "      <td>H04</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3049</th>\n",
       "      <td>61350d231d52c186</td>\n",
       "      <td>wire grid polarization</td>\n",
       "      <td>polarization</td>\n",
       "      <td>H04</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3050</th>\n",
       "      <td>59c0d33ccfb4f873</td>\n",
       "      <td>wire grid polarization</td>\n",
       "      <td>polarizing</td>\n",
       "      <td>H04</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3051</th>\n",
       "      <td>46017862b2545572</td>\n",
       "      <td>wire grid polarization</td>\n",
       "      <td>wire grid</td>\n",
       "      <td>H04</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3052</th>\n",
       "      <td>b4b17a1795a5ecc6</td>\n",
       "      <td>wire grid polarization</td>\n",
       "      <td>wire grid polarizers</td>\n",
       "      <td>H04</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3053</th>\n",
       "      <td>f1e1395c3c62f2ab</td>\n",
       "      <td>wire grid polarization</td>\n",
       "      <td>wire grid polarizing</td>\n",
       "      <td>H04</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3054 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                  anchor  \\\n",
       "0     6bdd1d05ffa3401e               abatement   \n",
       "1     a937ae580ea4bd97  achieve authentication   \n",
       "2     ea154b802bebc48a  achieve authentication   \n",
       "3     24eda11bf0665917  achieve authentication   \n",
       "4     14c027729175aedd  achieve authentication   \n",
       "...                ...                     ...   \n",
       "3049  61350d231d52c186  wire grid polarization   \n",
       "3050  59c0d33ccfb4f873  wire grid polarization   \n",
       "3051  46017862b2545572  wire grid polarization   \n",
       "3052  b4b17a1795a5ecc6  wire grid polarization   \n",
       "3053  f1e1395c3c62f2ab  wire grid polarization   \n",
       "\n",
       "                               target context  score  \n",
       "0                  emission abatement     H04   0.50  \n",
       "1                      achieve access     H04   0.75  \n",
       "2     achieve authentication accuracy     H04   0.50  \n",
       "3               achieve authorization     H04   0.75  \n",
       "4                        achieve goal     H04   0.00  \n",
       "...                               ...     ...    ...  \n",
       "3049                     polarization     H04   0.50  \n",
       "3050                       polarizing     H04   0.50  \n",
       "3051                        wire grid     H04   0.25  \n",
       "3052             wire grid polarizers     H04   1.00  \n",
       "3053             wire grid polarizing     H04   1.00  \n",
       "\n",
       "[3054 rows x 5 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kCeb1wK1RNk9"
   },
   "source": [
    "Vamos separar o dataset de treino e treino e teste, isso vai ajudar a fazer a validação dos dados do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "FC7mf3m95bQO"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train[['context', 'anchor', 'target']], train[['score']], test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "0ba81fb2"
   },
   "outputs": [],
   "source": [
    "context_train = X_train.context.values\n",
    "context_test = X_test.context.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8cdbc3ca",
    "outputId": "787778dd-b257-416d-e4b2-1852bc7bbfdb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['channel vectors', 'ftir spectrometer', 'image signal generation',\n",
       "       ..., 'hear aid parameters', 'instruction processing', 'el display'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchor_train = X_train.anchor.values\n",
    "anchor_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ff652906",
    "outputId": "af899733-760e-4805-95ae-f3d75d6cb6df"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['channel coefficient vector', 'scientific instrument',\n",
       "       'reverse image', ..., 'hear aid', 'data processing instruction',\n",
       "       'organic el display'], dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_train = X_train.target.values\n",
    "target_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d576aee1",
    "outputId": "43e5b802-7aca-4afe-f6ba-c416bb018657"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['cable securing', 'connected means', 'imaging axis',\n",
       "       'connected means', 'pulse width modulated control',\n",
       "       'cathode layer', 'imaging axis', 'receiver shaft',\n",
       "       'color difference signal', 'steering matrices', 'reduction factor',\n",
       "       'sheet supply roller', 'browsers', 'reduction factor',\n",
       "       'connected means', 'noncollinear', 'content analysis',\n",
       "       'apply to requests', 'blooming drains',\n",
       "       'display different pictures', 'neural stimulation', 'page file',\n",
       "       'imaging axis', 'cable securing', 'decompressor', 'based method',\n",
       "       'steering matrices', 'material item', 'instruction processing',\n",
       "       'page file', 'display different pictures', 'el display',\n",
       "       'place to set', 'channel vectors', 'determine from analysis',\n",
       "       'multiplexed data', 'sheet supply roller', 'form as body',\n",
       "       'request buffer', 'reflection type liquid crystal display',\n",
       "       'reflection type liquid crystal display', 'video lines', 'key arm',\n",
       "       'operator identification information', 'verifiable', 'page file',\n",
       "       'cathode layer', 'material item', 'browsers',\n",
       "       'color difference signal', 'offset table', 'channel vectors',\n",
       "       'cable securing', 'apply to requests', 'color difference signal',\n",
       "       'steering matrices', 'type parameter', 'provider networks',\n",
       "       'loudspeaker arrays', 'reflection type liquid crystal display',\n",
       "       'el display', 'different circumferential positions',\n",
       "       'adaptive linear', 'combination function', 'based interpolation',\n",
       "       'content analysis', 'type parameter', 'wire grid polarization',\n",
       "       'reflection type liquid crystal display', 'provide by terminals',\n",
       "       'el display', 'ecn', 'selected operation', 'calling card',\n",
       "       'average power ratio reduction', 'ftir spectrometer', 'page file',\n",
       "       'cathode layer', 'monocrystalline semiconductor',\n",
       "       'hardware blocks', 'split into flows', 'blooming drains',\n",
       "       'sheet supply roller', 'network load information',\n",
       "       'channel vectors', 'calling card', 'connected means', 'browsers',\n",
       "       'previously captured image', 'place to set', 'optical glasses',\n",
       "       'pen based computer', 'ingress buffer',\n",
       "       'electromagnetic radiation source', 'ftir spectrometer',\n",
       "       'apply to requests', 'portable radio communication',\n",
       "       'based interpolation', 'display different pictures', 'select unit',\n",
       "       'combination function', 'wire grid polarization',\n",
       "       'different circumferential positions', 'based interpolation',\n",
       "       'instruction processing', 'place to set',\n",
       "       'previously captured image', 'multiplexed data', 'verifiable',\n",
       "       'abatement', 'monocrystalline semiconductor', 'selected operation',\n",
       "       'pen based computer', 'content analysis', 'soft polymeric',\n",
       "       'portable radio communication', 'opc drum', 'browsers',\n",
       "       'broadband information', 'clear buffer', 'based method',\n",
       "       'generate in layer', 'adaptive linear', 'service processing unit',\n",
       "       'reflection type liquid crystal display', 'achieve authentication',\n",
       "       'arcuate means', 'content analysis', 'intermediate speed',\n",
       "       'blooming drains', 'el display', 'browsers',\n",
       "       'service processing unit', 'opc drum', 'send to control node',\n",
       "       'lamination method', 'combination function', 'cathode layer',\n",
       "       'switch swm', 'monocrystalline semiconductor',\n",
       "       'send to control node', 'laser beam optical', 'noncollinear',\n",
       "       'based interpolation', 'provider networks', 'optical glasses',\n",
       "       'pulse width modulated control', 'previously captured image',\n",
       "       'el display', 'offset table', 'instruction processing',\n",
       "       'portable radio communication', 'steering matrices',\n",
       "       'wire grid polarization', 'send to control node', 'killed',\n",
       "       'ftir spectrometer', 'monocrystalline semiconductor',\n",
       "       'duplex device', 'color difference signal', 'material item',\n",
       "       'encode form', 'service processing unit', 'request buffer',\n",
       "       'gutters', 'opc drum', 'fiber slack',\n",
       "       'combine with optical elements', 'color difference signal',\n",
       "       'ingress buffer', 'multiplexed data', 'linear change',\n",
       "       'receiver shaft', 'duplex device', 'network load information',\n",
       "       'split into flows', 'adaptive linear', 'previously captured image',\n",
       "       'blooming drains', 'broadband information',\n",
       "       'reflection type liquid crystal display', 'request buffer',\n",
       "       'wire grid polarization', 'operator identification information',\n",
       "       'verifiable', 'blooming drains', 'pen based computer',\n",
       "       'display different pictures', 'intermediate speed', 'place to set',\n",
       "       'combine with optical elements', 'provide by terminals',\n",
       "       'provide by terminals', 'loudspeaker arrays',\n",
       "       'monocrystalline semiconductor', 'ecn', 'opc drum',\n",
       "       'color difference signal', 'generate in layer',\n",
       "       'protocol component', 'request buffer', 'multiplexed data',\n",
       "       'el display', 'el display', 'transmit to platform',\n",
       "       'content analysis', 'connected means', 'key arm', 'based method',\n",
       "       'browsers', 'portable radio communication', 'pen based computer',\n",
       "       'el display', 'neural stimulation', 'gutters',\n",
       "       'transmit to platform', 'opc drum', 'transmit over interface',\n",
       "       'request buffer', 'color difference signal', 'neural stimulation',\n",
       "       'instruction processing', 'network load information',\n",
       "       'blooming drains', 'operating channel', 'based method',\n",
       "       'calling card', 'transmit over interface', 'arcuate means',\n",
       "       'neural stimulation', 'ecn', 'steering matrices', 'proper order',\n",
       "       'steering matrices', 'duplex device',\n",
       "       'pulse width modulated control', 'decompressor', 'deflect light',\n",
       "       'gutters', 'based method', 'ingress buffer', 'linear change',\n",
       "       'protocol component', 'ftir spectrometer', 'sheet supply roller',\n",
       "       'electromagnetic radiation source', 'opc drum', 'content analysis',\n",
       "       'place to set', 'protocol component', 'video lines',\n",
       "       'pen based computer', 'linear change', 'ingress buffer',\n",
       "       'wedge device', 'transmit over interface', 'wedge device',\n",
       "       'laser beam optical', 'instruction processing',\n",
       "       'apply to requests', 'proper order', 'color difference signal',\n",
       "       'page file', 'connected means', 'combination function',\n",
       "       'linear change', 'page file', 'sheet supply roller',\n",
       "       'operating channel', 'reduction factor',\n",
       "       'different circumferential positions', 'proper order', 'bandedge',\n",
       "       'request buffer', 'instruction processing',\n",
       "       'different circumferential positions', 'video lines',\n",
       "       'combine with optical elements', 'transmit to platform',\n",
       "       'pen based computer', 'service processing unit', 'hrb',\n",
       "       'lamination method', 'receiver shaft', 'offset table',\n",
       "       'electromagnetic radiation source', 'reduction factor',\n",
       "       'use physically unclonable functions', 'multiplexed data',\n",
       "       'instruction processing', 'image signal generation',\n",
       "       'portable radio communication', 'optical glasses',\n",
       "       'generate in layer', 'arcuate means', 'provide by terminals',\n",
       "       'pulse width modulated control', 'el display',\n",
       "       'different circumferential positions', 'instruction processing',\n",
       "       'imaging axis', 'encode form', 'service processing unit',\n",
       "       'network load information', 'transmit over interface', 'killed',\n",
       "       'cathode layer', 'wedge device', 'based interpolation',\n",
       "       'combination function', 'linear change', 'hardware blocks',\n",
       "       'split into flows', 'connected means', 'network load information',\n",
       "       'hear aid parameters', 'wire grid polarization',\n",
       "       'neural stimulation', 'duplex device', 'based method',\n",
       "       'offset table', 'reflection type liquid crystal display',\n",
       "       'offset table', 'cathode layer', 'decompressor', 'based method',\n",
       "       'collator', 'color difference signal', 'killed', 'key arm',\n",
       "       'provider networks', 'request buffer', 'multiplexed data',\n",
       "       'previously captured image', 'request buffer', 'page file',\n",
       "       'neural stimulation', 'based method', 'decompressor', 'page file',\n",
       "       'selected operation', 'reflection type liquid crystal display',\n",
       "       'operating channel', 'protocol component', 'adaptive linear',\n",
       "       'killed', 'transmit to platform', 'ecn', 'based method',\n",
       "       'operator identification information', 'calling card',\n",
       "       'cathode layer', 'reflection type liquid crystal display',\n",
       "       'imaging axis', 'lamination method', 'sheet supply roller',\n",
       "       'imaging axis', 'image signal generation', 'source channel',\n",
       "       'deflect light', 'hear aid parameters', 'provide by terminals',\n",
       "       'linear change', 'type parameter', 'pen based computer',\n",
       "       'arcuate means', 'form as body', 'provider networks', 'key arm',\n",
       "       'selected operation', 'operating channel', 'ingress buffer',\n",
       "       'multiplexed data', 'split into flows', 'multiplexed data',\n",
       "       'hardware blocks', 'different circumferential positions',\n",
       "       'imaging axis', 'operator identification information',\n",
       "       'determine from analysis', 'clear buffer', 'provide by terminals',\n",
       "       'reflection type liquid crystal display', 'hardware blocks',\n",
       "       'display different pictures', 'multiplexed data', 'cathode layer',\n",
       "       'selected operation', 'network load information',\n",
       "       'transmit to platform', 'image signal generation',\n",
       "       'multiplexed data', 'wire grid polarization', 'operating channel',\n",
       "       'sheet supply roller', 'network load information',\n",
       "       'broadband information', 'generate in layer',\n",
       "       'combine with optical elements', 'split into flows', 'page file',\n",
       "       'steering matrices', 'image signal generation',\n",
       "       'operating channel', 'image signal generation',\n",
       "       'lamination method', 'fiber slack', 'instruction processing',\n",
       "       'image signal generation', 'network load information',\n",
       "       'neural stimulation', 'portable radio communication',\n",
       "       'duplex device', 'type parameter',\n",
       "       'operator identification information', 'decompressor',\n",
       "       'pulse width modulated control', 'opc drum', 'sheet supply roller',\n",
       "       'encode form', 'video lines', 'collator', 'split into flows',\n",
       "       'encode form', 'use physically unclonable functions',\n",
       "       'request buffer', 'average power ratio reduction',\n",
       "       'ftir spectrometer', 'wire grid polarization', 'decompressor',\n",
       "       'gutters', 'calling card', 'gutters',\n",
       "       'pulse width modulated control',\n",
       "       'reflection type liquid crystal display', 'lamination method',\n",
       "       'ftir spectrometer', 'clear buffer', 'transmit to platform',\n",
       "       'duplex device', 'optical glasses', 'lamination method',\n",
       "       'display different pictures', 'multiplexed data', 'clear buffer',\n",
       "       'proper order', 'gutters', 'monocrystalline semiconductor',\n",
       "       'connected means', 'multiplexed data',\n",
       "       'monocrystalline semiconductor',\n",
       "       'use physically unclonable functions', 'page file', 'opc drum',\n",
       "       'arcuate means', 'network load information', 'type parameter',\n",
       "       'page file', 'duplex device', 'apply to requests',\n",
       "       'reflection type liquid crystal display', 'instruction processing',\n",
       "       'key arm', 'material item',\n",
       "       'reflection type liquid crystal display', 'content analysis',\n",
       "       'lamination method', 'calling card',\n",
       "       'operator identification information', 'type parameter',\n",
       "       'pen based computer', 'loudspeaker arrays', 'ingress buffer',\n",
       "       'imaging axis', 'clear buffer', 'multiplexed data',\n",
       "       'combination function', 'use physically unclonable functions',\n",
       "       'connected means', 'portable radio communication',\n",
       "       'service processing unit', 'source channel', 'form as body',\n",
       "       'ftir spectrometer', 'electromagnetic radiation source',\n",
       "       'physical transport medium', 'proper order',\n",
       "       'operator identification information', 'video lines',\n",
       "       'type parameter', 'multiplexed data', 'color difference signal',\n",
       "       'pen based computer', 'average power ratio reduction',\n",
       "       'video lines', 'verifiable', 'switch swm', 'duplex device',\n",
       "       'monocrystalline semiconductor', 'calling card',\n",
       "       'ftir spectrometer', 'broadband information',\n",
       "       'image signal generation', 'network load information',\n",
       "       'previously captured image', 'switch swm',\n",
       "       'different circumferential positions',\n",
       "       'average power ratio reduction', 'soft polymeric',\n",
       "       'reflection type liquid crystal display', 'clear buffer',\n",
       "       'broadband information', 'direct received light',\n",
       "       'electromagnetic radiation source', 'imaging axis',\n",
       "       'determine from analysis', 'combination function',\n",
       "       'provider networks', 'loudspeaker arrays', 'type parameter',\n",
       "       'el display', 'display different pictures', 'ftir spectrometer',\n",
       "       'ingress buffer', 'service processing unit', 'opc drum',\n",
       "       'channel vectors', 'based interpolation', 'material item',\n",
       "       'proper order', 'transmit over interface', 'provider networks',\n",
       "       'channel vectors', 'network load information', 'adaptive linear',\n",
       "       'wire grid polarization', 'adaptive linear', 'neural stimulation',\n",
       "       'el display', 'reduction factor', 'collator', 'proper order',\n",
       "       'combination function', 'adaptive linear', 'arcuate means', 'ecn',\n",
       "       'display different pictures', 'cable securing', 'offset table',\n",
       "       'type parameter', 'use physically unclonable functions',\n",
       "       'type parameter', 'request buffer', 'connected means', 'killed',\n",
       "       'reflection type liquid crystal display', 'reduction factor',\n",
       "       'wire grid polarization', 'portable radio communication',\n",
       "       'ingress buffer', 'decompressor', 'encode form',\n",
       "       'reflection type liquid crystal display', 'noncollinear',\n",
       "       'imaging axis', 'proper order', 'service processing unit',\n",
       "       'operating channel', 'broadband information', 'connected means',\n",
       "       'noncollinear', 'provider networks', 'operating channel',\n",
       "       'instruction processing', 'wire grid polarization',\n",
       "       'service processing unit', 'monocrystalline semiconductor',\n",
       "       'direct received light', 'different circumferential positions',\n",
       "       'network load information', 'cable securing', 'connected means',\n",
       "       'network load information', 'network load information',\n",
       "       'laser beam optical', 'reflection type liquid crystal display',\n",
       "       'channel vectors', 'hardware blocks',\n",
       "       'reflection type liquid crystal display', 'receiver shaft',\n",
       "       'el display', 'verifiable', 'operator identification information',\n",
       "       'sheet supply roller', 'service processing unit',\n",
       "       'hear aid parameters', 'based method', 'sheet supply roller',\n",
       "       'opc drum', 'send to control node', 'display different pictures',\n",
       "       'display different pictures', 'receiver shaft',\n",
       "       'instruction processing', 'provide by terminals', 'offset table',\n",
       "       'channel vectors', 'el display', 'provider networks',\n",
       "       'use physically unclonable functions',\n",
       "       'reflection type liquid crystal display'], dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchor_test = X_test.anchor.values\n",
    "anchor_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7836d674",
    "outputId": "dab032dc-fa03-41e9-b88c-c17d517ba3bf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['cable', 'distribution box', 'optical lens axis',\n",
       "       'rectifying elements', 'controller module', 'cathode substrate',\n",
       "       'microscope system', 'master shaft', 'chrominance',\n",
       "       'divergent beams', 'crest factor reduction', 'sheet transport',\n",
       "       'brochure templates', 'strength reduction factor',\n",
       "       'coupled transistors', 'non real numbers',\n",
       "       'video content analysis', 'request leave', 'blooming flower',\n",
       "       'display multiple pictures', 'neurological stimulation',\n",
       "       'confidential file', 'observation axis', 'unit folder',\n",
       "       'computer terminal', 'provide computer based method',\n",
       "       'plurality of steering matrices', 'wire material',\n",
       "       'instruction processing unit', 'page id', 'different display',\n",
       "       'illuminable', 'crowded place', 'channel response matrices',\n",
       "       'determine by gc analysis', 'data warehousing',\n",
       "       'sheet transport rollers', 'inner and outer surfaces',\n",
       "       'network request buffer', 'reflective mode lcd crystal display',\n",
       "       'reflection type lcd display', 'data', 'formation',\n",
       "       'operator id card', 'confirmable password', 'paging correction',\n",
       "       'cathode', 'ferromagnetic mold material', 'browsing center',\n",
       "       'difference in code', 'voltage offset table',\n",
       "       'respective channel vectors', 'unprotected network',\n",
       "       'apply request', 'luminance data', 'matrices calculator',\n",
       "       'control information', 'business network',\n",
       "       'acoustic transducers speakers', 'reflection type liquid',\n",
       "       'el display panel', 'opposite distal ends',\n",
       "       'non adaptive speckle filtering', 'logic function',\n",
       "       'signal interpolation', 'content detection test',\n",
       "       'variable parameter', 'consist of wire grid',\n",
       "       'camouflaged lamp housing', 'provide food', 'door panel',\n",
       "       'congestion episode', 'shift operation', 'calling console number',\n",
       "       'average ratio reduction', 'instrument cluster', 'file',\n",
       "       'thin film layer', 'mono si', 'processor', 'fusing images',\n",
       "       'anti blooming gates', 'paper supply roller',\n",
       "       'load of network traffic', 'signals', 'shopping card',\n",
       "       'connected without internet', 'internet',\n",
       "       'digital photographing camera', 'location to set', 'optical shop',\n",
       "       'stylus writing', 'ingress protection rating', 'source documents',\n",
       "       'spectrometer availability', 'apply request message',\n",
       "       'communication protocol', 'interpolation based',\n",
       "       'display distinct images', 'dust collection hose',\n",
       "       'combinational circuit', 'transparent polarization', 'flange',\n",
       "       'estimator', 'though processing', 'market place', 'mirror image',\n",
       "       'digital data', 'accessible', 'emission abatement',\n",
       "       'crystal jewellery', 'preprocessing step', 'notebook',\n",
       "       'analysis result data', 'hard polymeric backing',\n",
       "       'foldable processing', 'drum brake', 'web browser programs',\n",
       "       'wifi information', 'transparent buffer', 'process based',\n",
       "       'generate in layer', 'adaptive radiation', 'management server',\n",
       "       'reflection type display', 'denied access', 'source', 'attack',\n",
       "       'extremely slow speed', 'blooming', 'light el', 'ip address',\n",
       "       'cp requesting method', 'image photosensitive drum',\n",
       "       'forward market', 'sheet method', 'transport protocol',\n",
       "       'cathode material', 'single wire multi',\n",
       "       'non monocrystalline semiconductor', 'transfer to control node',\n",
       "       'printing', 'modes', 'linear pair', 'global network performance',\n",
       "       'suitable substrates', 'control', 'previously captured',\n",
       "       'el forming', 'offset', 'device instruction step', 'radio garden',\n",
       "       'beam former', 'wire grid polarizing', 'sending response',\n",
       "       'cold and heat', 'measuring instrument', 'single crystal',\n",
       "       'antipodal finline structures', 'color pencil', 'marked version',\n",
       "       'encode form video bit stream', 'passenger service unit', 'buffer',\n",
       "       'sewage channel', 'electrostatic photosensitive drum',\n",
       "       'fiber length', 'opaque matrix material layer', 'color wheel',\n",
       "       'ingress frame buffer', 'data science', 'progresses straight',\n",
       "       'receiver brush', 'duplex house', 'base station', 'split second',\n",
       "       'non linear model', 'feedback image', 'preventing blooming',\n",
       "       'bandwidth information', 'type of leaf', 'buffer media', 'wgp',\n",
       "       'session configuration information', 'confirmable',\n",
       "       'blooming pink flowers', 'remote device',\n",
       "       'sequence of different image', 'intermediate slow', 'set place',\n",
       "       'spects', 'provision by terminals', 'provide to input terminals',\n",
       "       'directional output transducers',\n",
       "       'single crystal semiconductor material',\n",
       "       'wide area computer network', 'drum kit', 'luminance signal',\n",
       "       'produce in layer', 'messaging product', 'access request',\n",
       "       'multiplexed', 'electro luminescent display', 'light emitting',\n",
       "       'sending to platform', 'code detection', 'rectifying means',\n",
       "       'arm button', 'program', 'loan application',\n",
       "       'portable communication devices', 'pen based tablet computer',\n",
       "       'fireworks display', 'medical', 'gutters bootstrap 4',\n",
       "       'receiving from platform', 'image bearing drum',\n",
       "       'network interface layer', 'request queue',\n",
       "       'color difference signals', 'neural stimulator',\n",
       "       'strict instruction', 'secret information', 'blooming transistor',\n",
       "       'water channel', 'methods', 'calling free number',\n",
       "       'receive over interface', 'means of transport', 'neural signal',\n",
       "       'multi user computer', 'transmission spans', 'buffer', 'antennas',\n",
       "       'terminating devices', 'pulse control signals', 'decoder',\n",
       "       'reflect beams', 'clean water sump',\n",
       "       'artificial intelligence based method', 'output fifo',\n",
       "       'uniform variation', 'marginal gateway protocol', 'ftir spectra',\n",
       "       'sheet feed transport roller', 'excitation radiation device',\n",
       "       'copier device part', 'analysis', 'set of instructional signals',\n",
       "       'assessment year', 'video field', 'camera pen computer',\n",
       "       'linear fashion', 'traffic management', 'wedge layer',\n",
       "       'receiver device', 'prism device', 'laser', 'processor',\n",
       "       'sampling rate', 'aptly positioned', 'video signals', 'page data',\n",
       "       'connection portion', 'transporter', 'corresponding change',\n",
       "       'server', 'sheet drawing roller', 'access operable',\n",
       "       'allocation value', 'top positions', 'order', 'maximum peak',\n",
       "       'database in order', 'pipeline instruction', 'equal intervals',\n",
       "       'sections', 'optical component combination', 'transmit to node',\n",
       "       'remote receiver device', 'requesting module', 'frequency hrb',\n",
       "       'remove cover layer', 'shaft ring', 'current offset matrix',\n",
       "       'radiant energy source', 'shift', 'physical one way functions',\n",
       "       'transmission', 'execution unit', 'signal generation',\n",
       "       'radio signal', 'organic resins', 'silicon wafer without layers',\n",
       "       'imparting an arcuate shape', 'provide by return', 'pulse rate',\n",
       "       'ole', 'random positions', 'voice instruction processing',\n",
       "       'optical axis', 'decryption code', 'media server',\n",
       "       'upset load carriers', 'transmit over bus', 'live streaming',\n",
       "       'anode layer', 'device surface', 'known good component',\n",
       "       'electronic combination', 'zero uniform change',\n",
       "       'hardware design block', 'fusing operators', 'switching means',\n",
       "       'network least congestion information', 'aid enzyme',\n",
       "       'grid polarizer', 'neural stimulation device', 'floor control',\n",
       "       'protocol based', 'table glass', 'camouflaged circuit structure',\n",
       "       'table value', 'thin active layer', 'compressor',\n",
       "       'without using any method', 'collation apparatus',\n",
       "       'color ratio signal', 'killed defects', 'key', 'provider ns',\n",
       "       'request for leave', 'multiplexed data bus',\n",
       "       'previous captured picture', 'receive request', 'paging file size',\n",
       "       'implant', 'circuit capable', 'decompression unit',\n",
       "       'document file', 'user selection control', 'liquid type device',\n",
       "       'inactive channel selecting resource', 'component',\n",
       "       'adaptive linear filter', 'shot', 'send to platform',\n",
       "       'edge compute nodes', 'power switching method',\n",
       "       'information identification', 'calling card number',\n",
       "       'cathode conductive layer', 'reflective type crystal', 'axis',\n",
       "       'printing', 'multi cutter paper tube', 'imaging direction',\n",
       "       'image correction', 'systematic linear', 'converge optical path',\n",
       "       'hearing aid profiles', 'terminal velocity', 'change',\n",
       "       'restricted access window', 'notebook computer',\n",
       "       'circular ring sector', 'form as single part',\n",
       "       'provider network ns', 'key structure', 'selected single strategy',\n",
       "       'operation channel', 'buffer action', 'signal demultiplexer',\n",
       "       'river flows', 'big data', 'receiver chain',\n",
       "       'imaginary cylindrical surface', 'optical of axis',\n",
       "       'session terminal configuration information', 'circuit analysis',\n",
       "       'polymer', 'provide shelter', 'reflective type crystal display',\n",
       "       'hardware machine', 'different electrodes', 'data handling',\n",
       "       'cathode active material', 'selected menu',\n",
       "       'network traffic load data', 'transfer to platform',\n",
       "       'video signal generator', 'multiplexed communication signal',\n",
       "       'wire grid polarizing', 'general channel',\n",
       "       'sheets transport feed roller', 'statistical information',\n",
       "       'wide monitor', 'generate in substrate',\n",
       "       'combine with optical components', 'disparate flows',\n",
       "       'master file', 'spans', 'picture channel', 'operating channels',\n",
       "       'image capturing sensor', 'adhesive member',\n",
       "       'wireless communication', 'card processing', 'hydrogen generation',\n",
       "       'base information', 'de energize the laser', 'ethernet cable',\n",
       "       'hospital devices', 'control message',\n",
       "       'operator terminal configuration information', 'genetic algorithm',\n",
       "       'wall width', 'drum', 'paper feed portion',\n",
       "       'interframe encode form', 'signal transmission line',\n",
       "       'collating apparatus', 'divide into separate flows',\n",
       "       'predetermined encode form', 'function of heart',\n",
       "       'outgoing buffer', 'peak power suppression', 'spectroscopy',\n",
       "       'polarization', 'encryption mechanism', 'electro optic medium',\n",
       "       'telephone number', 'transparent gutter', 'voice control',\n",
       "       'reflection mode liquid crystal', 'printing process',\n",
       "       'real life service', 'flush buffer', 'transmit from platform',\n",
       "       'pc device', 'medical equipment', 'spray process method',\n",
       "       'display control unit', 'multiplex digital data',\n",
       "       'calculation device', 'proper subset', 'place value',\n",
       "       'monocrystalline diamonds', 'voltage means elements',\n",
       "       'broadcast demultiplexer', 'electronics chip', 'use dustbin',\n",
       "       'page document file', 'photoconductive medium', 'arcuate ligament',\n",
       "       'number of information', 'types of soil', 'ram', 'sip',\n",
       "       'electronic ordering systems', 'reflective liquid crystal',\n",
       "       'request receiving', 'weighted arm', 'composite material',\n",
       "       'liquid crystal display', 'content extraction', 'basic method',\n",
       "       'prepaid calling card', 'information', 'indication field',\n",
       "       'computer module', 'directional transducers',\n",
       "       'ingress storage buffer', 'optical of direction', 'clear skies',\n",
       "       'demux processors', 'non linear combining function', 'puf panel',\n",
       "       'connecting electronic devices', 'mobile telephone',\n",
       "       'cp requesting unit', 'turbo', 'sealing form', 'phase modulator',\n",
       "       'optical source', 'coax cable', 'disarranged winding',\n",
       "       'operator identification data', 'signature signals',\n",
       "       'associated parameter', 'transmission data', 'color signals',\n",
       "       'pen based computer system', 'two stage index modulation',\n",
       "       'video line selector', 'easily verifiable', 'optical switch swm',\n",
       "       'duplex mf', 'semiconductor shortage', 'quick call',\n",
       "       'separate lines', 'typical information',\n",
       "       'digital image processing', 'base station load',\n",
       "       'previously captured by image device', 'switchless combiner',\n",
       "       'predetermined intervals', 'peak reducing signal',\n",
       "       'hard polymeric constituent', 'reflective type liquid',\n",
       "       'transparent base', 'configuration', 'received light signal',\n",
       "       'illumination element', 'optic axis',\n",
       "       'determine by image analysis', 'function', 'social network',\n",
       "       'microphone', 'type of food', 'artwork display',\n",
       "       'provisional single image', 'infrared ir', 'shared buffer pool',\n",
       "       'sound processing unit', 'photoconductor', 'complex channel gains',\n",
       "       'extrapolation techniques', 'original version', 'disarranged data',\n",
       "       'over cooked', 'internet provider', 'reception vectors',\n",
       "       'load information', 'adaptive linear equalization',\n",
       "       'non polarized wire grid', 'adapting', 'electrical stimulation',\n",
       "       'el emitting', 'shift values', 'simulation device',\n",
       "       'unchanged order', 'combination function generation', 'network',\n",
       "       'means of communication', 'message processing',\n",
       "       'display varied pictures', 'electrical', 'correction storing unit',\n",
       "       'data type parameter', 'physically challenged', 'reflection',\n",
       "       'request storage list', 'connector', 'live video packets', 'tv',\n",
       "       'reduction', 'threaded union nut',\n",
       "       'portable wireless communication device', 'buffer solution',\n",
       "       'decomposed leaves', 'encode in form',\n",
       "       'reflection matrix type crystal display', 'perpendicular modes',\n",
       "       'fluoroscopy method', 'wrong order', 'standard unit of current',\n",
       "       'trans receiving operating channel', 'conventional information',\n",
       "       'disconnector pole', 'perpendicular polarization directions',\n",
       "       'service provider networks', 'active frequency',\n",
       "       'instruction transmitting step', 'pbs polarizing',\n",
       "       'cp ns requesting program', 'solar photovoltaic cells',\n",
       "       'direct path', 'equi angular intervals', 'network of information',\n",
       "       'cable gland', 'components', 'network station', 'congestion data',\n",
       "       'optical fiber laser', 'reflection liquid crystal', 'channel',\n",
       "       'arithmetic units', 'reflective type liquid display',\n",
       "       'drive shaft', 'electroluminescent', 'car theft prevention',\n",
       "       'identifying the operator of the terminal',\n",
       "       'sheets transport feed rollers', 'connection control unit',\n",
       "       'wireless connectivity', 'implemented system',\n",
       "       'sheet transport roller', 'printer', 'send mail',\n",
       "       'items on display', 'display same pictures', 'motor shaft',\n",
       "       'device information process', 'provide by user terminals',\n",
       "       'offset table address', 'respective channel',\n",
       "       'electroluminescent emitting', 'server network',\n",
       "       'physical fitness', 'bright crystal device'], dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_test = X_test.target.values\n",
    "target_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "941222e3"
   },
   "outputs": [],
   "source": [
    "hub_url = \"https://tfhub.dev/google/sentence-t5/st5-base/1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "MZOpxPZ_i59B"
   },
   "outputs": [],
   "source": [
    "def aplica_t5(text):\n",
    "    tensor_list = []\n",
    "\n",
    "    not_finish = True\n",
    "\n",
    "    i = 0\n",
    "    while not_finish:\n",
    "        tensor_list.append(encoder(text[i:i+1])[0][0])\n",
    "        if i == len(text) - 1:\n",
    "            not_finish = False\n",
    "        i +=1\n",
    "        print(i)\n",
    "    print(\"Fim\")\n",
    "  \n",
    "    return tensor_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yT6ZlOV-RpH-"
   },
   "source": [
    "## Aplicando o sentence t5 \n",
    "\n",
    "Agora vamos aplicar o sentence t5, um encoder de texto, em cada uma das nossas colunas de texto. Isso fará com que as sentenças e paragráfos mapeados se tornem um vetor de dimensão 768. Isso terá uma extrema importância pros modelos pois eles não conseguiriam entender um texto em seu formato original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1124d514",
    "outputId": "d8aa4847-450c-4c96-abb1-b1bd4f0f1145"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Importing a function (__inference_<lambda>_9720) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_<lambda>_3354) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_<lambda>_6722) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    }
   ],
   "source": [
    "encoder = hub.KerasLayer(hub_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MzGpaxVtjbYe",
    "outputId": "19269234-9e86-44cb-b113-67f5040a55c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "1001\n",
      "1002\n",
      "1003\n",
      "1004\n",
      "1005\n",
      "1006\n",
      "1007\n",
      "1008\n",
      "1009\n",
      "1010\n",
      "1011\n",
      "1012\n",
      "1013\n",
      "1014\n",
      "1015\n",
      "1016\n",
      "1017\n",
      "1018\n",
      "1019\n",
      "1020\n",
      "1021\n",
      "1022\n",
      "1023\n",
      "1024\n",
      "1025\n",
      "1026\n",
      "1027\n",
      "1028\n",
      "1029\n",
      "1030\n",
      "1031\n",
      "1032\n",
      "1033\n",
      "1034\n",
      "1035\n",
      "1036\n",
      "1037\n",
      "1038\n",
      "1039\n",
      "1040\n",
      "1041\n",
      "1042\n",
      "1043\n",
      "1044\n",
      "1045\n",
      "1046\n",
      "1047\n",
      "1048\n",
      "1049\n",
      "1050\n",
      "1051\n",
      "1052\n",
      "1053\n",
      "1054\n",
      "1055\n",
      "1056\n",
      "1057\n",
      "1058\n",
      "1059\n",
      "1060\n",
      "1061\n",
      "1062\n",
      "1063\n",
      "1064\n",
      "1065\n",
      "1066\n",
      "1067\n",
      "1068\n",
      "1069\n",
      "1070\n",
      "1071\n",
      "1072\n",
      "1073\n",
      "1074\n",
      "1075\n",
      "1076\n",
      "1077\n",
      "1078\n",
      "1079\n",
      "1080\n",
      "1081\n",
      "1082\n",
      "1083\n",
      "1084\n",
      "1085\n",
      "1086\n",
      "1087\n",
      "1088\n",
      "1089\n",
      "1090\n",
      "1091\n",
      "1092\n",
      "1093\n",
      "1094\n",
      "1095\n",
      "1096\n",
      "1097\n",
      "1098\n",
      "1099\n",
      "1100\n",
      "1101\n",
      "1102\n",
      "1103\n",
      "1104\n",
      "1105\n",
      "1106\n",
      "1107\n",
      "1108\n",
      "1109\n",
      "1110\n",
      "1111\n",
      "1112\n",
      "1113\n",
      "1114\n",
      "1115\n",
      "1116\n",
      "1117\n",
      "1118\n",
      "1119\n",
      "1120\n",
      "1121\n",
      "1122\n",
      "1123\n",
      "1124\n",
      "1125\n",
      "1126\n",
      "1127\n",
      "1128\n",
      "1129\n",
      "1130\n",
      "1131\n",
      "1132\n",
      "1133\n",
      "1134\n",
      "1135\n",
      "1136\n",
      "1137\n",
      "1138\n",
      "1139\n",
      "1140\n",
      "1141\n",
      "1142\n",
      "1143\n",
      "1144\n",
      "1145\n",
      "1146\n",
      "1147\n",
      "1148\n",
      "1149\n",
      "1150\n",
      "1151\n",
      "1152\n",
      "1153\n",
      "1154\n",
      "1155\n",
      "1156\n",
      "1157\n",
      "1158\n",
      "1159\n",
      "1160\n",
      "1161\n",
      "1162\n",
      "1163\n",
      "1164\n",
      "1165\n",
      "1166\n",
      "1167\n",
      "1168\n",
      "1169\n",
      "1170\n",
      "1171\n",
      "1172\n",
      "1173\n",
      "1174\n",
      "1175\n",
      "1176\n",
      "1177\n",
      "1178\n",
      "1179\n",
      "1180\n",
      "1181\n",
      "1182\n",
      "1183\n",
      "1184\n",
      "1185\n",
      "1186\n",
      "1187\n",
      "1188\n",
      "1189\n",
      "1190\n",
      "1191\n",
      "1192\n",
      "1193\n",
      "1194\n",
      "1195\n",
      "1196\n",
      "1197\n",
      "1198\n",
      "1199\n",
      "1200\n",
      "1201\n",
      "1202\n",
      "1203\n",
      "1204\n",
      "1205\n",
      "1206\n",
      "1207\n",
      "1208\n",
      "1209\n",
      "1210\n",
      "1211\n",
      "1212\n",
      "1213\n",
      "1214\n",
      "1215\n",
      "1216\n",
      "1217\n",
      "1218\n",
      "1219\n",
      "1220\n",
      "1221\n",
      "1222\n",
      "1223\n",
      "1224\n",
      "1225\n",
      "1226\n",
      "1227\n",
      "1228\n",
      "1229\n",
      "1230\n",
      "1231\n",
      "1232\n",
      "1233\n",
      "1234\n",
      "1235\n",
      "1236\n",
      "1237\n",
      "1238\n",
      "1239\n",
      "1240\n",
      "1241\n",
      "1242\n",
      "1243\n",
      "1244\n",
      "1245\n",
      "1246\n",
      "1247\n",
      "1248\n",
      "1249\n",
      "1250\n",
      "1251\n",
      "1252\n",
      "1253\n",
      "1254\n",
      "1255\n",
      "1256\n",
      "1257\n",
      "1258\n",
      "1259\n",
      "1260\n",
      "1261\n",
      "1262\n",
      "1263\n",
      "1264\n",
      "1265\n",
      "1266\n",
      "1267\n",
      "1268\n",
      "1269\n",
      "1270\n",
      "1271\n",
      "1272\n",
      "1273\n",
      "1274\n",
      "1275\n",
      "1276\n",
      "1277\n",
      "1278\n",
      "1279\n",
      "1280\n",
      "1281\n",
      "1282\n",
      "1283\n",
      "1284\n",
      "1285\n",
      "1286\n",
      "1287\n",
      "1288\n",
      "1289\n",
      "1290\n",
      "1291\n",
      "1292\n",
      "1293\n",
      "1294\n",
      "1295\n",
      "1296\n",
      "1297\n",
      "1298\n",
      "1299\n",
      "1300\n",
      "1301\n",
      "1302\n",
      "1303\n",
      "1304\n",
      "1305\n",
      "1306\n",
      "1307\n",
      "1308\n",
      "1309\n",
      "1310\n",
      "1311\n",
      "1312\n",
      "1313\n",
      "1314\n",
      "1315\n",
      "1316\n",
      "1317\n",
      "1318\n",
      "1319\n",
      "1320\n",
      "1321\n",
      "1322\n",
      "1323\n",
      "1324\n",
      "1325\n",
      "1326\n",
      "1327\n",
      "1328\n",
      "1329\n",
      "1330\n",
      "1331\n",
      "1332\n",
      "1333\n",
      "1334\n",
      "1335\n",
      "1336\n",
      "1337\n",
      "1338\n",
      "1339\n",
      "1340\n",
      "1341\n",
      "1342\n",
      "1343\n",
      "1344\n",
      "1345\n",
      "1346\n",
      "1347\n",
      "1348\n",
      "1349\n",
      "1350\n",
      "1351\n",
      "1352\n",
      "1353\n",
      "1354\n",
      "1355\n",
      "1356\n",
      "1357\n",
      "1358\n",
      "1359\n",
      "1360\n",
      "1361\n",
      "1362\n",
      "1363\n",
      "1364\n",
      "1365\n",
      "1366\n",
      "1367\n",
      "1368\n",
      "1369\n",
      "1370\n",
      "1371\n",
      "1372\n",
      "1373\n",
      "1374\n",
      "1375\n",
      "1376\n",
      "1377\n",
      "1378\n",
      "1379\n",
      "1380\n",
      "1381\n",
      "1382\n",
      "1383\n",
      "1384\n",
      "1385\n",
      "1386\n",
      "1387\n",
      "1388\n",
      "1389\n",
      "1390\n",
      "1391\n",
      "1392\n",
      "1393\n",
      "1394\n",
      "1395\n",
      "1396\n",
      "1397\n",
      "1398\n",
      "1399\n",
      "1400\n",
      "1401\n",
      "1402\n",
      "1403\n",
      "1404\n",
      "1405\n",
      "1406\n",
      "1407\n",
      "1408\n",
      "1409\n",
      "1410\n",
      "1411\n",
      "1412\n",
      "1413\n",
      "1414\n",
      "1415\n",
      "1416\n",
      "1417\n",
      "1418\n",
      "1419\n",
      "1420\n",
      "1421\n",
      "1422\n",
      "1423\n",
      "1424\n",
      "1425\n",
      "1426\n",
      "1427\n",
      "1428\n",
      "1429\n",
      "1430\n",
      "1431\n",
      "1432\n",
      "1433\n",
      "1434\n",
      "1435\n",
      "1436\n",
      "1437\n",
      "1438\n",
      "1439\n",
      "1440\n",
      "1441\n",
      "1442\n",
      "1443\n",
      "1444\n",
      "1445\n",
      "1446\n",
      "1447\n",
      "1448\n",
      "1449\n",
      "1450\n",
      "1451\n",
      "1452\n",
      "1453\n",
      "1454\n",
      "1455\n",
      "1456\n",
      "1457\n",
      "1458\n",
      "1459\n",
      "1460\n",
      "1461\n",
      "1462\n",
      "1463\n",
      "1464\n",
      "1465\n",
      "1466\n",
      "1467\n",
      "1468\n",
      "1469\n",
      "1470\n",
      "1471\n",
      "1472\n",
      "1473\n",
      "1474\n",
      "1475\n",
      "1476\n",
      "1477\n",
      "1478\n",
      "1479\n",
      "1480\n",
      "1481\n",
      "1482\n",
      "1483\n",
      "1484\n",
      "1485\n",
      "1486\n",
      "1487\n",
      "1488\n",
      "1489\n",
      "1490\n",
      "1491\n",
      "1492\n",
      "1493\n",
      "1494\n",
      "1495\n",
      "1496\n",
      "1497\n",
      "1498\n",
      "1499\n",
      "1500\n",
      "1501\n",
      "1502\n",
      "1503\n",
      "1504\n",
      "1505\n",
      "1506\n",
      "1507\n",
      "1508\n",
      "1509\n",
      "1510\n",
      "1511\n",
      "1512\n",
      "1513\n",
      "1514\n",
      "1515\n",
      "1516\n",
      "1517\n",
      "1518\n",
      "1519\n",
      "1520\n",
      "1521\n",
      "1522\n",
      "1523\n",
      "1524\n",
      "1525\n",
      "1526\n",
      "1527\n",
      "1528\n",
      "1529\n",
      "1530\n",
      "1531\n",
      "1532\n",
      "1533\n",
      "1534\n",
      "1535\n",
      "1536\n",
      "1537\n",
      "1538\n",
      "1539\n",
      "1540\n",
      "1541\n",
      "1542\n",
      "1543\n",
      "1544\n",
      "1545\n",
      "1546\n",
      "1547\n",
      "1548\n",
      "1549\n",
      "1550\n",
      "1551\n",
      "1552\n",
      "1553\n",
      "1554\n",
      "1555\n",
      "1556\n",
      "1557\n",
      "1558\n",
      "1559\n",
      "1560\n",
      "1561\n",
      "1562\n",
      "1563\n",
      "1564\n",
      "1565\n",
      "1566\n",
      "1567\n",
      "1568\n",
      "1569\n",
      "1570\n",
      "1571\n",
      "1572\n",
      "1573\n",
      "1574\n",
      "1575\n",
      "1576\n",
      "1577\n",
      "1578\n",
      "1579\n",
      "1580\n",
      "1581\n",
      "1582\n",
      "1583\n",
      "1584\n",
      "1585\n",
      "1586\n",
      "1587\n",
      "1588\n",
      "1589\n",
      "1590\n",
      "1591\n",
      "1592\n",
      "1593\n",
      "1594\n",
      "1595\n",
      "1596\n",
      "1597\n",
      "1598\n",
      "1599\n",
      "1600\n",
      "1601\n",
      "1602\n",
      "1603\n",
      "1604\n",
      "1605\n",
      "1606\n",
      "1607\n",
      "1608\n",
      "1609\n",
      "1610\n",
      "1611\n",
      "1612\n",
      "1613\n",
      "1614\n",
      "1615\n",
      "1616\n",
      "1617\n",
      "1618\n",
      "1619\n",
      "1620\n",
      "1621\n",
      "1622\n",
      "1623\n",
      "1624\n",
      "1625\n",
      "1626\n",
      "1627\n",
      "1628\n",
      "1629\n",
      "1630\n",
      "1631\n",
      "1632\n",
      "1633\n",
      "1634\n",
      "1635\n",
      "1636\n",
      "1637\n",
      "1638\n",
      "1639\n",
      "1640\n",
      "1641\n",
      "1642\n",
      "1643\n",
      "1644\n",
      "1645\n",
      "1646\n",
      "1647\n",
      "1648\n",
      "1649\n",
      "1650\n",
      "1651\n",
      "1652\n",
      "1653\n",
      "1654\n",
      "1655\n",
      "1656\n",
      "1657\n",
      "1658\n",
      "1659\n",
      "1660\n",
      "1661\n",
      "1662\n",
      "1663\n",
      "1664\n",
      "1665\n",
      "1666\n",
      "1667\n",
      "1668\n",
      "1669\n",
      "1670\n",
      "1671\n",
      "1672\n",
      "1673\n",
      "1674\n",
      "1675\n",
      "1676\n",
      "1677\n",
      "1678\n",
      "1679\n",
      "1680\n",
      "1681\n",
      "1682\n",
      "1683\n",
      "1684\n",
      "1685\n",
      "1686\n",
      "1687\n",
      "1688\n",
      "1689\n",
      "1690\n",
      "1691\n",
      "1692\n",
      "1693\n",
      "1694\n",
      "1695\n",
      "1696\n",
      "1697\n",
      "1698\n",
      "1699\n",
      "1700\n",
      "1701\n",
      "1702\n",
      "1703\n",
      "1704\n",
      "1705\n",
      "1706\n",
      "1707\n",
      "1708\n",
      "1709\n",
      "1710\n",
      "1711\n",
      "1712\n",
      "1713\n",
      "1714\n",
      "1715\n",
      "1716\n",
      "1717\n",
      "1718\n",
      "1719\n",
      "1720\n",
      "1721\n",
      "1722\n",
      "1723\n",
      "1724\n",
      "1725\n",
      "1726\n",
      "1727\n",
      "1728\n",
      "1729\n",
      "1730\n",
      "1731\n",
      "1732\n",
      "1733\n",
      "1734\n",
      "1735\n",
      "1736\n",
      "1737\n",
      "1738\n",
      "1739\n",
      "1740\n",
      "1741\n",
      "1742\n",
      "1743\n",
      "1744\n",
      "1745\n",
      "1746\n",
      "1747\n",
      "1748\n",
      "1749\n",
      "1750\n",
      "1751\n",
      "1752\n",
      "1753\n",
      "1754\n",
      "1755\n",
      "1756\n",
      "1757\n",
      "1758\n",
      "1759\n",
      "1760\n",
      "1761\n",
      "1762\n",
      "1763\n",
      "1764\n",
      "1765\n",
      "1766\n",
      "1767\n",
      "1768\n",
      "1769\n",
      "1770\n",
      "1771\n",
      "1772\n",
      "1773\n",
      "1774\n",
      "1775\n",
      "1776\n",
      "1777\n",
      "1778\n",
      "1779\n",
      "1780\n",
      "1781\n",
      "1782\n",
      "1783\n",
      "1784\n",
      "1785\n",
      "1786\n",
      "1787\n",
      "1788\n",
      "1789\n",
      "1790\n",
      "1791\n",
      "1792\n",
      "1793\n",
      "1794\n",
      "1795\n",
      "1796\n",
      "1797\n",
      "1798\n",
      "1799\n",
      "1800\n",
      "1801\n",
      "1802\n",
      "1803\n",
      "1804\n",
      "1805\n",
      "1806\n",
      "1807\n",
      "1808\n",
      "1809\n",
      "1810\n",
      "1811\n",
      "1812\n",
      "1813\n",
      "1814\n",
      "1815\n",
      "1816\n",
      "1817\n",
      "1818\n",
      "1819\n",
      "1820\n",
      "1821\n",
      "1822\n",
      "1823\n",
      "1824\n",
      "1825\n",
      "1826\n",
      "1827\n",
      "1828\n",
      "1829\n",
      "1830\n",
      "1831\n",
      "1832\n",
      "1833\n",
      "1834\n",
      "1835\n",
      "1836\n",
      "1837\n",
      "1838\n",
      "1839\n",
      "1840\n",
      "1841\n",
      "1842\n",
      "1843\n",
      "1844\n",
      "1845\n",
      "1846\n",
      "1847\n",
      "1848\n",
      "1849\n",
      "1850\n",
      "1851\n",
      "1852\n",
      "1853\n",
      "1854\n",
      "1855\n",
      "1856\n",
      "1857\n",
      "1858\n",
      "1859\n",
      "1860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1861\n",
      "1862\n",
      "1863\n",
      "1864\n",
      "1865\n",
      "1866\n",
      "1867\n",
      "1868\n",
      "1869\n",
      "1870\n",
      "1871\n",
      "1872\n",
      "1873\n",
      "1874\n",
      "1875\n",
      "1876\n",
      "1877\n",
      "1878\n",
      "1879\n",
      "1880\n",
      "1881\n",
      "1882\n",
      "1883\n",
      "1884\n",
      "1885\n",
      "1886\n",
      "1887\n",
      "1888\n",
      "1889\n",
      "1890\n",
      "1891\n",
      "1892\n",
      "1893\n",
      "1894\n",
      "1895\n",
      "1896\n",
      "1897\n",
      "1898\n",
      "1899\n",
      "1900\n",
      "1901\n",
      "1902\n",
      "1903\n",
      "1904\n",
      "1905\n",
      "1906\n",
      "1907\n",
      "1908\n",
      "1909\n",
      "1910\n",
      "1911\n",
      "1912\n",
      "1913\n",
      "1914\n",
      "1915\n",
      "1916\n",
      "1917\n",
      "1918\n",
      "1919\n",
      "1920\n",
      "1921\n",
      "1922\n",
      "1923\n",
      "1924\n",
      "1925\n",
      "1926\n",
      "1927\n",
      "1928\n",
      "1929\n",
      "1930\n",
      "1931\n",
      "1932\n",
      "1933\n",
      "1934\n",
      "1935\n",
      "1936\n",
      "1937\n",
      "1938\n",
      "1939\n",
      "1940\n",
      "1941\n",
      "1942\n",
      "1943\n",
      "1944\n",
      "1945\n",
      "1946\n",
      "1947\n",
      "1948\n",
      "1949\n",
      "1950\n",
      "1951\n",
      "1952\n",
      "1953\n",
      "1954\n",
      "1955\n",
      "1956\n",
      "1957\n",
      "1958\n",
      "1959\n",
      "1960\n",
      "1961\n",
      "1962\n",
      "1963\n",
      "1964\n",
      "1965\n",
      "1966\n",
      "1967\n",
      "1968\n",
      "1969\n",
      "1970\n",
      "1971\n",
      "1972\n",
      "1973\n",
      "1974\n",
      "1975\n",
      "1976\n",
      "1977\n",
      "1978\n",
      "1979\n",
      "1980\n",
      "1981\n",
      "1982\n",
      "1983\n",
      "1984\n",
      "1985\n",
      "1986\n",
      "1987\n",
      "1988\n",
      "1989\n",
      "1990\n",
      "1991\n",
      "1992\n",
      "1993\n",
      "1994\n",
      "1995\n",
      "1996\n",
      "1997\n",
      "1998\n",
      "1999\n",
      "2000\n",
      "2001\n",
      "2002\n",
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n",
      "2009\n",
      "2010\n",
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "2017\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "2021\n",
      "2022\n",
      "2023\n",
      "2024\n",
      "2025\n",
      "2026\n",
      "2027\n",
      "2028\n",
      "2029\n",
      "2030\n",
      "2031\n",
      "2032\n",
      "2033\n",
      "2034\n",
      "2035\n",
      "2036\n",
      "2037\n",
      "2038\n",
      "2039\n",
      "2040\n",
      "2041\n",
      "2042\n",
      "2043\n",
      "2044\n",
      "2045\n",
      "2046\n",
      "2047\n",
      "2048\n",
      "2049\n",
      "2050\n",
      "2051\n",
      "2052\n",
      "2053\n",
      "2054\n",
      "2055\n",
      "2056\n",
      "2057\n",
      "2058\n",
      "2059\n",
      "2060\n",
      "2061\n",
      "2062\n",
      "2063\n",
      "2064\n",
      "2065\n",
      "2066\n",
      "2067\n",
      "2068\n",
      "2069\n",
      "2070\n",
      "2071\n",
      "2072\n",
      "2073\n",
      "2074\n",
      "2075\n",
      "2076\n",
      "2077\n",
      "2078\n",
      "2079\n",
      "2080\n",
      "2081\n",
      "2082\n",
      "2083\n",
      "2084\n",
      "2085\n",
      "2086\n",
      "2087\n",
      "2088\n",
      "2089\n",
      "2090\n",
      "2091\n",
      "2092\n",
      "2093\n",
      "2094\n",
      "2095\n",
      "2096\n",
      "2097\n",
      "2098\n",
      "2099\n",
      "2100\n",
      "2101\n",
      "2102\n",
      "2103\n",
      "2104\n",
      "2105\n",
      "2106\n",
      "2107\n",
      "2108\n",
      "2109\n",
      "2110\n",
      "2111\n",
      "2112\n",
      "2113\n",
      "2114\n",
      "2115\n",
      "2116\n",
      "2117\n",
      "2118\n",
      "2119\n",
      "2120\n",
      "2121\n",
      "2122\n",
      "2123\n",
      "2124\n",
      "2125\n",
      "2126\n",
      "2127\n",
      "2128\n",
      "2129\n",
      "2130\n",
      "2131\n",
      "2132\n",
      "2133\n",
      "2134\n",
      "2135\n",
      "2136\n",
      "2137\n",
      "2138\n",
      "2139\n",
      "2140\n",
      "2141\n",
      "2142\n",
      "2143\n",
      "2144\n",
      "2145\n",
      "2146\n",
      "2147\n",
      "2148\n",
      "2149\n",
      "2150\n",
      "2151\n",
      "2152\n",
      "2153\n",
      "2154\n",
      "2155\n",
      "2156\n",
      "2157\n",
      "2158\n",
      "2159\n",
      "2160\n",
      "2161\n",
      "2162\n",
      "2163\n",
      "2164\n",
      "2165\n",
      "2166\n",
      "2167\n",
      "2168\n",
      "2169\n",
      "2170\n",
      "2171\n",
      "2172\n",
      "2173\n",
      "2174\n",
      "2175\n",
      "2176\n",
      "2177\n",
      "2178\n",
      "2179\n",
      "2180\n",
      "2181\n",
      "2182\n",
      "2183\n",
      "2184\n",
      "2185\n",
      "2186\n",
      "2187\n",
      "2188\n",
      "2189\n",
      "2190\n",
      "2191\n",
      "2192\n",
      "2193\n",
      "2194\n",
      "2195\n",
      "2196\n",
      "2197\n",
      "2198\n",
      "2199\n",
      "2200\n",
      "2201\n",
      "2202\n",
      "2203\n",
      "2204\n",
      "2205\n",
      "2206\n",
      "2207\n",
      "2208\n",
      "2209\n",
      "2210\n",
      "2211\n",
      "2212\n",
      "2213\n",
      "2214\n",
      "2215\n",
      "2216\n",
      "2217\n",
      "2218\n",
      "2219\n",
      "2220\n",
      "2221\n",
      "2222\n",
      "2223\n",
      "2224\n",
      "2225\n",
      "2226\n",
      "2227\n",
      "2228\n",
      "2229\n",
      "2230\n",
      "2231\n",
      "2232\n",
      "2233\n",
      "2234\n",
      "2235\n",
      "2236\n",
      "2237\n",
      "2238\n",
      "2239\n",
      "2240\n",
      "2241\n",
      "2242\n",
      "2243\n",
      "2244\n",
      "2245\n",
      "2246\n",
      "2247\n",
      "2248\n",
      "2249\n",
      "2250\n",
      "2251\n",
      "2252\n",
      "2253\n",
      "2254\n",
      "2255\n",
      "2256\n",
      "2257\n",
      "2258\n",
      "2259\n",
      "2260\n",
      "2261\n",
      "2262\n",
      "2263\n",
      "2264\n",
      "2265\n",
      "2266\n",
      "2267\n",
      "2268\n",
      "2269\n",
      "2270\n",
      "2271\n",
      "2272\n",
      "2273\n",
      "2274\n",
      "2275\n",
      "2276\n",
      "2277\n",
      "2278\n",
      "2279\n",
      "2280\n",
      "2281\n",
      "2282\n",
      "2283\n",
      "2284\n",
      "2285\n",
      "2286\n",
      "2287\n",
      "2288\n",
      "2289\n",
      "2290\n",
      "2291\n",
      "2292\n",
      "2293\n",
      "2294\n",
      "2295\n",
      "2296\n",
      "2297\n",
      "2298\n",
      "2299\n",
      "2300\n",
      "2301\n",
      "2302\n",
      "2303\n",
      "2304\n",
      "2305\n",
      "2306\n",
      "2307\n",
      "2308\n",
      "2309\n",
      "2310\n",
      "2311\n",
      "2312\n",
      "2313\n",
      "2314\n",
      "2315\n",
      "2316\n",
      "2317\n",
      "2318\n",
      "2319\n",
      "2320\n",
      "2321\n",
      "2322\n",
      "2323\n",
      "2324\n",
      "2325\n",
      "2326\n",
      "2327\n",
      "2328\n",
      "2329\n",
      "2330\n",
      "2331\n",
      "2332\n",
      "2333\n",
      "2334\n",
      "2335\n",
      "2336\n",
      "2337\n",
      "2338\n",
      "2339\n",
      "2340\n",
      "2341\n",
      "2342\n",
      "2343\n",
      "2344\n",
      "2345\n",
      "2346\n",
      "2347\n",
      "2348\n",
      "2349\n",
      "2350\n",
      "2351\n",
      "2352\n",
      "2353\n",
      "2354\n",
      "2355\n",
      "2356\n",
      "2357\n",
      "2358\n",
      "2359\n",
      "2360\n",
      "2361\n",
      "2362\n",
      "2363\n",
      "2364\n",
      "2365\n",
      "2366\n",
      "2367\n",
      "2368\n",
      "2369\n",
      "2370\n",
      "2371\n",
      "2372\n",
      "2373\n",
      "2374\n",
      "2375\n",
      "2376\n",
      "2377\n",
      "2378\n",
      "2379\n",
      "2380\n",
      "2381\n",
      "2382\n",
      "2383\n",
      "2384\n",
      "2385\n",
      "2386\n",
      "2387\n",
      "2388\n",
      "2389\n",
      "2390\n",
      "2391\n",
      "2392\n",
      "2393\n",
      "2394\n",
      "2395\n",
      "2396\n",
      "2397\n",
      "2398\n",
      "2399\n",
      "2400\n",
      "2401\n",
      "2402\n",
      "2403\n",
      "2404\n",
      "2405\n",
      "2406\n",
      "2407\n",
      "2408\n",
      "2409\n",
      "2410\n",
      "2411\n",
      "2412\n",
      "2413\n",
      "2414\n",
      "2415\n",
      "2416\n",
      "2417\n",
      "2418\n",
      "2419\n",
      "2420\n",
      "2421\n",
      "2422\n",
      "2423\n",
      "2424\n",
      "2425\n",
      "2426\n",
      "2427\n",
      "2428\n",
      "2429\n",
      "2430\n",
      "2431\n",
      "2432\n",
      "2433\n",
      "2434\n",
      "2435\n",
      "2436\n",
      "2437\n",
      "2438\n",
      "2439\n",
      "2440\n",
      "2441\n",
      "2442\n",
      "2443\n",
      "Fim\n"
     ]
    }
   ],
   "source": [
    "tensor_list_anchor_train = [item.numpy() for item in aplica_t5(anchor_train)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jddOvnKykVrK",
    "outputId": "41db5c35-5cb2-4bd9-f7c0-df98ee42be16",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "1001\n",
      "1002\n",
      "1003\n",
      "1004\n",
      "1005\n",
      "1006\n",
      "1007\n",
      "1008\n",
      "1009\n",
      "1010\n",
      "1011\n",
      "1012\n",
      "1013\n",
      "1014\n",
      "1015\n",
      "1016\n",
      "1017\n",
      "1018\n",
      "1019\n",
      "1020\n",
      "1021\n",
      "1022\n",
      "1023\n",
      "1024\n",
      "1025\n",
      "1026\n",
      "1027\n",
      "1028\n",
      "1029\n",
      "1030\n",
      "1031\n",
      "1032\n",
      "1033\n",
      "1034\n",
      "1035\n",
      "1036\n",
      "1037\n",
      "1038\n",
      "1039\n",
      "1040\n",
      "1041\n",
      "1042\n",
      "1043\n",
      "1044\n",
      "1045\n",
      "1046\n",
      "1047\n",
      "1048\n",
      "1049\n",
      "1050\n",
      "1051\n",
      "1052\n",
      "1053\n",
      "1054\n",
      "1055\n",
      "1056\n",
      "1057\n",
      "1058\n",
      "1059\n",
      "1060\n",
      "1061\n",
      "1062\n",
      "1063\n",
      "1064\n",
      "1065\n",
      "1066\n",
      "1067\n",
      "1068\n",
      "1069\n",
      "1070\n",
      "1071\n",
      "1072\n",
      "1073\n",
      "1074\n",
      "1075\n",
      "1076\n",
      "1077\n",
      "1078\n",
      "1079\n",
      "1080\n",
      "1081\n",
      "1082\n",
      "1083\n",
      "1084\n",
      "1085\n",
      "1086\n",
      "1087\n",
      "1088\n",
      "1089\n",
      "1090\n",
      "1091\n",
      "1092\n",
      "1093\n",
      "1094\n",
      "1095\n",
      "1096\n",
      "1097\n",
      "1098\n",
      "1099\n",
      "1100\n",
      "1101\n",
      "1102\n",
      "1103\n",
      "1104\n",
      "1105\n",
      "1106\n",
      "1107\n",
      "1108\n",
      "1109\n",
      "1110\n",
      "1111\n",
      "1112\n",
      "1113\n",
      "1114\n",
      "1115\n",
      "1116\n",
      "1117\n",
      "1118\n",
      "1119\n",
      "1120\n",
      "1121\n",
      "1122\n",
      "1123\n",
      "1124\n",
      "1125\n",
      "1126\n",
      "1127\n",
      "1128\n",
      "1129\n",
      "1130\n",
      "1131\n",
      "1132\n",
      "1133\n",
      "1134\n",
      "1135\n",
      "1136\n",
      "1137\n",
      "1138\n",
      "1139\n",
      "1140\n",
      "1141\n",
      "1142\n",
      "1143\n",
      "1144\n",
      "1145\n",
      "1146\n",
      "1147\n",
      "1148\n",
      "1149\n",
      "1150\n",
      "1151\n",
      "1152\n",
      "1153\n",
      "1154\n",
      "1155\n",
      "1156\n",
      "1157\n",
      "1158\n",
      "1159\n",
      "1160\n",
      "1161\n",
      "1162\n",
      "1163\n",
      "1164\n",
      "1165\n",
      "1166\n",
      "1167\n",
      "1168\n",
      "1169\n",
      "1170\n",
      "1171\n",
      "1172\n",
      "1173\n",
      "1174\n",
      "1175\n",
      "1176\n",
      "1177\n",
      "1178\n",
      "1179\n",
      "1180\n",
      "1181\n",
      "1182\n",
      "1183\n",
      "1184\n",
      "1185\n",
      "1186\n",
      "1187\n",
      "1188\n",
      "1189\n",
      "1190\n",
      "1191\n",
      "1192\n",
      "1193\n",
      "1194\n",
      "1195\n",
      "1196\n",
      "1197\n",
      "1198\n",
      "1199\n",
      "1200\n",
      "1201\n",
      "1202\n",
      "1203\n",
      "1204\n",
      "1205\n",
      "1206\n",
      "1207\n",
      "1208\n",
      "1209\n",
      "1210\n",
      "1211\n",
      "1212\n",
      "1213\n",
      "1214\n",
      "1215\n",
      "1216\n",
      "1217\n",
      "1218\n",
      "1219\n",
      "1220\n",
      "1221\n",
      "1222\n",
      "1223\n",
      "1224\n",
      "1225\n",
      "1226\n",
      "1227\n",
      "1228\n",
      "1229\n",
      "1230\n",
      "1231\n",
      "1232\n",
      "1233\n",
      "1234\n",
      "1235\n",
      "1236\n",
      "1237\n",
      "1238\n",
      "1239\n",
      "1240\n",
      "1241\n",
      "1242\n",
      "1243\n",
      "1244\n",
      "1245\n",
      "1246\n",
      "1247\n",
      "1248\n",
      "1249\n",
      "1250\n",
      "1251\n",
      "1252\n",
      "1253\n",
      "1254\n",
      "1255\n",
      "1256\n",
      "1257\n",
      "1258\n",
      "1259\n",
      "1260\n",
      "1261\n",
      "1262\n",
      "1263\n",
      "1264\n",
      "1265\n",
      "1266\n",
      "1267\n",
      "1268\n",
      "1269\n",
      "1270\n",
      "1271\n",
      "1272\n",
      "1273\n",
      "1274\n",
      "1275\n",
      "1276\n",
      "1277\n",
      "1278\n",
      "1279\n",
      "1280\n",
      "1281\n",
      "1282\n",
      "1283\n",
      "1284\n",
      "1285\n",
      "1286\n",
      "1287\n",
      "1288\n",
      "1289\n",
      "1290\n",
      "1291\n",
      "1292\n",
      "1293\n",
      "1294\n",
      "1295\n",
      "1296\n",
      "1297\n",
      "1298\n",
      "1299\n",
      "1300\n",
      "1301\n",
      "1302\n",
      "1303\n",
      "1304\n",
      "1305\n",
      "1306\n",
      "1307\n",
      "1308\n",
      "1309\n",
      "1310\n",
      "1311\n",
      "1312\n",
      "1313\n",
      "1314\n",
      "1315\n",
      "1316\n",
      "1317\n",
      "1318\n",
      "1319\n",
      "1320\n",
      "1321\n",
      "1322\n",
      "1323\n",
      "1324\n",
      "1325\n",
      "1326\n",
      "1327\n",
      "1328\n",
      "1329\n",
      "1330\n",
      "1331\n",
      "1332\n",
      "1333\n",
      "1334\n",
      "1335\n",
      "1336\n",
      "1337\n",
      "1338\n",
      "1339\n",
      "1340\n",
      "1341\n",
      "1342\n",
      "1343\n",
      "1344\n",
      "1345\n",
      "1346\n",
      "1347\n",
      "1348\n",
      "1349\n",
      "1350\n",
      "1351\n",
      "1352\n",
      "1353\n",
      "1354\n",
      "1355\n",
      "1356\n",
      "1357\n",
      "1358\n",
      "1359\n",
      "1360\n",
      "1361\n",
      "1362\n",
      "1363\n",
      "1364\n",
      "1365\n",
      "1366\n",
      "1367\n",
      "1368\n",
      "1369\n",
      "1370\n",
      "1371\n",
      "1372\n",
      "1373\n",
      "1374\n",
      "1375\n",
      "1376\n",
      "1377\n",
      "1378\n",
      "1379\n",
      "1380\n",
      "1381\n",
      "1382\n",
      "1383\n",
      "1384\n",
      "1385\n",
      "1386\n",
      "1387\n",
      "1388\n",
      "1389\n",
      "1390\n",
      "1391\n",
      "1392\n",
      "1393\n",
      "1394\n",
      "1395\n",
      "1396\n",
      "1397\n",
      "1398\n",
      "1399\n",
      "1400\n",
      "1401\n",
      "1402\n",
      "1403\n",
      "1404\n",
      "1405\n",
      "1406\n",
      "1407\n",
      "1408\n",
      "1409\n",
      "1410\n",
      "1411\n",
      "1412\n",
      "1413\n",
      "1414\n",
      "1415\n",
      "1416\n",
      "1417\n",
      "1418\n",
      "1419\n",
      "1420\n",
      "1421\n",
      "1422\n",
      "1423\n",
      "1424\n",
      "1425\n",
      "1426\n",
      "1427\n",
      "1428\n",
      "1429\n",
      "1430\n",
      "1431\n",
      "1432\n",
      "1433\n",
      "1434\n",
      "1435\n",
      "1436\n",
      "1437\n",
      "1438\n",
      "1439\n",
      "1440\n",
      "1441\n",
      "1442\n",
      "1443\n",
      "1444\n",
      "1445\n",
      "1446\n",
      "1447\n",
      "1448\n",
      "1449\n",
      "1450\n",
      "1451\n",
      "1452\n",
      "1453\n",
      "1454\n",
      "1455\n",
      "1456\n",
      "1457\n",
      "1458\n",
      "1459\n",
      "1460\n",
      "1461\n",
      "1462\n",
      "1463\n",
      "1464\n",
      "1465\n",
      "1466\n",
      "1467\n",
      "1468\n",
      "1469\n",
      "1470\n",
      "1471\n",
      "1472\n",
      "1473\n",
      "1474\n",
      "1475\n",
      "1476\n",
      "1477\n",
      "1478\n",
      "1479\n",
      "1480\n",
      "1481\n",
      "1482\n",
      "1483\n",
      "1484\n",
      "1485\n",
      "1486\n",
      "1487\n",
      "1488\n",
      "1489\n",
      "1490\n",
      "1491\n",
      "1492\n",
      "1493\n",
      "1494\n",
      "1495\n",
      "1496\n",
      "1497\n",
      "1498\n",
      "1499\n",
      "1500\n",
      "1501\n",
      "1502\n",
      "1503\n",
      "1504\n",
      "1505\n",
      "1506\n",
      "1507\n",
      "1508\n",
      "1509\n",
      "1510\n",
      "1511\n",
      "1512\n",
      "1513\n",
      "1514\n",
      "1515\n",
      "1516\n",
      "1517\n",
      "1518\n",
      "1519\n",
      "1520\n",
      "1521\n",
      "1522\n",
      "1523\n",
      "1524\n",
      "1525\n",
      "1526\n",
      "1527\n",
      "1528\n",
      "1529\n",
      "1530\n",
      "1531\n",
      "1532\n",
      "1533\n",
      "1534\n",
      "1535\n",
      "1536\n",
      "1537\n",
      "1538\n",
      "1539\n",
      "1540\n",
      "1541\n",
      "1542\n",
      "1543\n",
      "1544\n",
      "1545\n",
      "1546\n",
      "1547\n",
      "1548\n",
      "1549\n",
      "1550\n",
      "1551\n",
      "1552\n",
      "1553\n",
      "1554\n",
      "1555\n",
      "1556\n",
      "1557\n",
      "1558\n",
      "1559\n",
      "1560\n",
      "1561\n",
      "1562\n",
      "1563\n",
      "1564\n",
      "1565\n",
      "1566\n",
      "1567\n",
      "1568\n",
      "1569\n",
      "1570\n",
      "1571\n",
      "1572\n",
      "1573\n",
      "1574\n",
      "1575\n",
      "1576\n",
      "1577\n",
      "1578\n",
      "1579\n",
      "1580\n",
      "1581\n",
      "1582\n",
      "1583\n",
      "1584\n",
      "1585\n",
      "1586\n",
      "1587\n",
      "1588\n",
      "1589\n",
      "1590\n",
      "1591\n",
      "1592\n",
      "1593\n",
      "1594\n",
      "1595\n",
      "1596\n",
      "1597\n",
      "1598\n",
      "1599\n",
      "1600\n",
      "1601\n",
      "1602\n",
      "1603\n",
      "1604\n",
      "1605\n",
      "1606\n",
      "1607\n",
      "1608\n",
      "1609\n",
      "1610\n",
      "1611\n",
      "1612\n",
      "1613\n",
      "1614\n",
      "1615\n",
      "1616\n",
      "1617\n",
      "1618\n",
      "1619\n",
      "1620\n",
      "1621\n",
      "1622\n",
      "1623\n",
      "1624\n",
      "1625\n",
      "1626\n",
      "1627\n",
      "1628\n",
      "1629\n",
      "1630\n",
      "1631\n",
      "1632\n",
      "1633\n",
      "1634\n",
      "1635\n",
      "1636\n",
      "1637\n",
      "1638\n",
      "1639\n",
      "1640\n",
      "1641\n",
      "1642\n",
      "1643\n",
      "1644\n",
      "1645\n",
      "1646\n",
      "1647\n",
      "1648\n",
      "1649\n",
      "1650\n",
      "1651\n",
      "1652\n",
      "1653\n",
      "1654\n",
      "1655\n",
      "1656\n",
      "1657\n",
      "1658\n",
      "1659\n",
      "1660\n",
      "1661\n",
      "1662\n",
      "1663\n",
      "1664\n",
      "1665\n",
      "1666\n",
      "1667\n",
      "1668\n",
      "1669\n",
      "1670\n",
      "1671\n",
      "1672\n",
      "1673\n",
      "1674\n",
      "1675\n",
      "1676\n",
      "1677\n",
      "1678\n",
      "1679\n",
      "1680\n",
      "1681\n",
      "1682\n",
      "1683\n",
      "1684\n",
      "1685\n",
      "1686\n",
      "1687\n",
      "1688\n",
      "1689\n",
      "1690\n",
      "1691\n",
      "1692\n",
      "1693\n",
      "1694\n",
      "1695\n",
      "1696\n",
      "1697\n",
      "1698\n",
      "1699\n",
      "1700\n",
      "1701\n",
      "1702\n",
      "1703\n",
      "1704\n",
      "1705\n",
      "1706\n",
      "1707\n",
      "1708\n",
      "1709\n",
      "1710\n",
      "1711\n",
      "1712\n",
      "1713\n",
      "1714\n",
      "1715\n",
      "1716\n",
      "1717\n",
      "1718\n",
      "1719\n",
      "1720\n",
      "1721\n",
      "1722\n",
      "1723\n",
      "1724\n",
      "1725\n",
      "1726\n",
      "1727\n",
      "1728\n",
      "1729\n",
      "1730\n",
      "1731\n",
      "1732\n",
      "1733\n",
      "1734\n",
      "1735\n",
      "1736\n",
      "1737\n",
      "1738\n",
      "1739\n",
      "1740\n",
      "1741\n",
      "1742\n",
      "1743\n",
      "1744\n",
      "1745\n",
      "1746\n",
      "1747\n",
      "1748\n",
      "1749\n",
      "1750\n",
      "1751\n",
      "1752\n",
      "1753\n",
      "1754\n",
      "1755\n",
      "1756\n",
      "1757\n",
      "1758\n",
      "1759\n",
      "1760\n",
      "1761\n",
      "1762\n",
      "1763\n",
      "1764\n",
      "1765\n",
      "1766\n",
      "1767\n",
      "1768\n",
      "1769\n",
      "1770\n",
      "1771\n",
      "1772\n",
      "1773\n",
      "1774\n",
      "1775\n",
      "1776\n",
      "1777\n",
      "1778\n",
      "1779\n",
      "1780\n",
      "1781\n",
      "1782\n",
      "1783\n",
      "1784\n",
      "1785\n",
      "1786\n",
      "1787\n",
      "1788\n",
      "1789\n",
      "1790\n",
      "1791\n",
      "1792\n",
      "1793\n",
      "1794\n",
      "1795\n",
      "1796\n",
      "1797\n",
      "1798\n",
      "1799\n",
      "1800\n",
      "1801\n",
      "1802\n",
      "1803\n",
      "1804\n",
      "1805\n",
      "1806\n",
      "1807\n",
      "1808\n",
      "1809\n",
      "1810\n",
      "1811\n",
      "1812\n",
      "1813\n",
      "1814\n",
      "1815\n",
      "1816\n",
      "1817\n",
      "1818\n",
      "1819\n",
      "1820\n",
      "1821\n",
      "1822\n",
      "1823\n",
      "1824\n",
      "1825\n",
      "1826\n",
      "1827\n",
      "1828\n",
      "1829\n",
      "1830\n",
      "1831\n",
      "1832\n",
      "1833\n",
      "1834\n",
      "1835\n",
      "1836\n",
      "1837\n",
      "1838\n",
      "1839\n",
      "1840\n",
      "1841\n",
      "1842\n",
      "1843\n",
      "1844\n",
      "1845\n",
      "1846\n",
      "1847\n",
      "1848\n",
      "1849\n",
      "1850\n",
      "1851\n",
      "1852\n",
      "1853\n",
      "1854\n",
      "1855\n",
      "1856\n",
      "1857\n",
      "1858\n",
      "1859\n",
      "1860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1861\n",
      "1862\n",
      "1863\n",
      "1864\n",
      "1865\n",
      "1866\n",
      "1867\n",
      "1868\n",
      "1869\n",
      "1870\n",
      "1871\n",
      "1872\n",
      "1873\n",
      "1874\n",
      "1875\n",
      "1876\n",
      "1877\n",
      "1878\n",
      "1879\n",
      "1880\n",
      "1881\n",
      "1882\n",
      "1883\n",
      "1884\n",
      "1885\n",
      "1886\n",
      "1887\n",
      "1888\n",
      "1889\n",
      "1890\n",
      "1891\n",
      "1892\n",
      "1893\n",
      "1894\n",
      "1895\n",
      "1896\n",
      "1897\n",
      "1898\n",
      "1899\n",
      "1900\n",
      "1901\n",
      "1902\n",
      "1903\n",
      "1904\n",
      "1905\n",
      "1906\n",
      "1907\n",
      "1908\n",
      "1909\n",
      "1910\n",
      "1911\n",
      "1912\n",
      "1913\n",
      "1914\n",
      "1915\n",
      "1916\n",
      "1917\n",
      "1918\n",
      "1919\n",
      "1920\n",
      "1921\n",
      "1922\n",
      "1923\n",
      "1924\n",
      "1925\n",
      "1926\n",
      "1927\n",
      "1928\n",
      "1929\n",
      "1930\n",
      "1931\n",
      "1932\n",
      "1933\n",
      "1934\n",
      "1935\n",
      "1936\n",
      "1937\n",
      "1938\n",
      "1939\n",
      "1940\n",
      "1941\n",
      "1942\n",
      "1943\n",
      "1944\n",
      "1945\n",
      "1946\n",
      "1947\n",
      "1948\n",
      "1949\n",
      "1950\n",
      "1951\n",
      "1952\n",
      "1953\n",
      "1954\n",
      "1955\n",
      "1956\n",
      "1957\n",
      "1958\n",
      "1959\n",
      "1960\n",
      "1961\n",
      "1962\n",
      "1963\n",
      "1964\n",
      "1965\n",
      "1966\n",
      "1967\n",
      "1968\n",
      "1969\n",
      "1970\n",
      "1971\n",
      "1972\n",
      "1973\n",
      "1974\n",
      "1975\n",
      "1976\n",
      "1977\n",
      "1978\n",
      "1979\n",
      "1980\n",
      "1981\n",
      "1982\n",
      "1983\n",
      "1984\n",
      "1985\n",
      "1986\n",
      "1987\n",
      "1988\n",
      "1989\n",
      "1990\n",
      "1991\n",
      "1992\n",
      "1993\n",
      "1994\n",
      "1995\n",
      "1996\n",
      "1997\n",
      "1998\n",
      "1999\n",
      "2000\n",
      "2001\n",
      "2002\n",
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n",
      "2009\n",
      "2010\n",
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "2017\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "2021\n",
      "2022\n",
      "2023\n",
      "2024\n",
      "2025\n",
      "2026\n",
      "2027\n",
      "2028\n",
      "2029\n",
      "2030\n",
      "2031\n",
      "2032\n",
      "2033\n",
      "2034\n",
      "2035\n",
      "2036\n",
      "2037\n",
      "2038\n",
      "2039\n",
      "2040\n",
      "2041\n",
      "2042\n",
      "2043\n",
      "2044\n",
      "2045\n",
      "2046\n",
      "2047\n",
      "2048\n",
      "2049\n",
      "2050\n",
      "2051\n",
      "2052\n",
      "2053\n",
      "2054\n",
      "2055\n",
      "2056\n",
      "2057\n",
      "2058\n",
      "2059\n",
      "2060\n",
      "2061\n",
      "2062\n",
      "2063\n",
      "2064\n",
      "2065\n",
      "2066\n",
      "2067\n",
      "2068\n",
      "2069\n",
      "2070\n",
      "2071\n",
      "2072\n",
      "2073\n",
      "2074\n",
      "2075\n",
      "2076\n",
      "2077\n",
      "2078\n",
      "2079\n",
      "2080\n",
      "2081\n",
      "2082\n",
      "2083\n",
      "2084\n",
      "2085\n",
      "2086\n",
      "2087\n",
      "2088\n",
      "2089\n",
      "2090\n",
      "2091\n",
      "2092\n",
      "2093\n",
      "2094\n",
      "2095\n",
      "2096\n",
      "2097\n",
      "2098\n",
      "2099\n",
      "2100\n",
      "2101\n",
      "2102\n",
      "2103\n",
      "2104\n",
      "2105\n",
      "2106\n",
      "2107\n",
      "2108\n",
      "2109\n",
      "2110\n",
      "2111\n",
      "2112\n",
      "2113\n",
      "2114\n",
      "2115\n",
      "2116\n",
      "2117\n",
      "2118\n",
      "2119\n",
      "2120\n",
      "2121\n",
      "2122\n",
      "2123\n",
      "2124\n",
      "2125\n",
      "2126\n",
      "2127\n",
      "2128\n",
      "2129\n",
      "2130\n",
      "2131\n",
      "2132\n",
      "2133\n",
      "2134\n",
      "2135\n",
      "2136\n",
      "2137\n",
      "2138\n",
      "2139\n",
      "2140\n",
      "2141\n",
      "2142\n",
      "2143\n",
      "2144\n",
      "2145\n",
      "2146\n",
      "2147\n",
      "2148\n",
      "2149\n",
      "2150\n",
      "2151\n",
      "2152\n",
      "2153\n",
      "2154\n",
      "2155\n",
      "2156\n",
      "2157\n",
      "2158\n",
      "2159\n",
      "2160\n",
      "2161\n",
      "2162\n",
      "2163\n",
      "2164\n",
      "2165\n",
      "2166\n",
      "2167\n",
      "2168\n",
      "2169\n",
      "2170\n",
      "2171\n",
      "2172\n",
      "2173\n",
      "2174\n",
      "2175\n",
      "2176\n",
      "2177\n",
      "2178\n",
      "2179\n",
      "2180\n",
      "2181\n",
      "2182\n",
      "2183\n",
      "2184\n",
      "2185\n",
      "2186\n",
      "2187\n",
      "2188\n",
      "2189\n",
      "2190\n",
      "2191\n",
      "2192\n",
      "2193\n",
      "2194\n",
      "2195\n",
      "2196\n",
      "2197\n",
      "2198\n",
      "2199\n",
      "2200\n",
      "2201\n",
      "2202\n",
      "2203\n",
      "2204\n",
      "2205\n",
      "2206\n",
      "2207\n",
      "2208\n",
      "2209\n",
      "2210\n",
      "2211\n",
      "2212\n",
      "2213\n",
      "2214\n",
      "2215\n",
      "2216\n",
      "2217\n",
      "2218\n",
      "2219\n",
      "2220\n",
      "2221\n",
      "2222\n",
      "2223\n",
      "2224\n",
      "2225\n",
      "2226\n",
      "2227\n",
      "2228\n",
      "2229\n",
      "2230\n",
      "2231\n",
      "2232\n",
      "2233\n",
      "2234\n",
      "2235\n",
      "2236\n",
      "2237\n",
      "2238\n",
      "2239\n",
      "2240\n",
      "2241\n",
      "2242\n",
      "2243\n",
      "2244\n",
      "2245\n",
      "2246\n",
      "2247\n",
      "2248\n",
      "2249\n",
      "2250\n",
      "2251\n",
      "2252\n",
      "2253\n",
      "2254\n",
      "2255\n",
      "2256\n",
      "2257\n",
      "2258\n",
      "2259\n",
      "2260\n",
      "2261\n",
      "2262\n",
      "2263\n",
      "2264\n",
      "2265\n",
      "2266\n",
      "2267\n",
      "2268\n",
      "2269\n",
      "2270\n",
      "2271\n",
      "2272\n",
      "2273\n",
      "2274\n",
      "2275\n",
      "2276\n",
      "2277\n",
      "2278\n",
      "2279\n",
      "2280\n",
      "2281\n",
      "2282\n",
      "2283\n",
      "2284\n",
      "2285\n",
      "2286\n",
      "2287\n",
      "2288\n",
      "2289\n",
      "2290\n",
      "2291\n",
      "2292\n",
      "2293\n",
      "2294\n",
      "2295\n",
      "2296\n",
      "2297\n",
      "2298\n",
      "2299\n",
      "2300\n",
      "2301\n",
      "2302\n",
      "2303\n",
      "2304\n",
      "2305\n",
      "2306\n",
      "2307\n",
      "2308\n",
      "2309\n",
      "2310\n",
      "2311\n",
      "2312\n",
      "2313\n",
      "2314\n",
      "2315\n",
      "2316\n",
      "2317\n",
      "2318\n",
      "2319\n",
      "2320\n",
      "2321\n",
      "2322\n",
      "2323\n",
      "2324\n",
      "2325\n",
      "2326\n",
      "2327\n",
      "2328\n",
      "2329\n",
      "2330\n",
      "2331\n",
      "2332\n",
      "2333\n",
      "2334\n",
      "2335\n",
      "2336\n",
      "2337\n",
      "2338\n",
      "2339\n",
      "2340\n",
      "2341\n",
      "2342\n",
      "2343\n",
      "2344\n",
      "2345\n",
      "2346\n",
      "2347\n",
      "2348\n",
      "2349\n",
      "2350\n",
      "2351\n",
      "2352\n",
      "2353\n",
      "2354\n",
      "2355\n",
      "2356\n",
      "2357\n",
      "2358\n",
      "2359\n",
      "2360\n",
      "2361\n",
      "2362\n",
      "2363\n",
      "2364\n",
      "2365\n",
      "2366\n",
      "2367\n",
      "2368\n",
      "2369\n",
      "2370\n",
      "2371\n",
      "2372\n",
      "2373\n",
      "2374\n",
      "2375\n",
      "2376\n",
      "2377\n",
      "2378\n",
      "2379\n",
      "2380\n",
      "2381\n",
      "2382\n",
      "2383\n",
      "2384\n",
      "2385\n",
      "2386\n",
      "2387\n",
      "2388\n",
      "2389\n",
      "2390\n",
      "2391\n",
      "2392\n",
      "2393\n",
      "2394\n",
      "2395\n",
      "2396\n",
      "2397\n",
      "2398\n",
      "2399\n",
      "2400\n",
      "2401\n",
      "2402\n",
      "2403\n",
      "2404\n",
      "2405\n",
      "2406\n",
      "2407\n",
      "2408\n",
      "2409\n",
      "2410\n",
      "2411\n",
      "2412\n",
      "2413\n",
      "2414\n",
      "2415\n",
      "2416\n",
      "2417\n",
      "2418\n",
      "2419\n",
      "2420\n",
      "2421\n",
      "2422\n",
      "2423\n",
      "2424\n",
      "2425\n",
      "2426\n",
      "2427\n",
      "2428\n",
      "2429\n",
      "2430\n",
      "2431\n",
      "2432\n",
      "2433\n",
      "2434\n",
      "2435\n",
      "2436\n",
      "2437\n",
      "2438\n",
      "2439\n",
      "2440\n",
      "2441\n",
      "2442\n",
      "2443\n",
      "Fim\n"
     ]
    }
   ],
   "source": [
    "tensor_list_target_train = [item.numpy() for item in aplica_t5(target_train)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O9fkVw6hlx3g",
    "outputId": "e9517ef1-208b-4cc5-ba3f-8f135368ecdc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "Fim\n"
     ]
    }
   ],
   "source": [
    "tensor_list_anchor_test = [item.numpy() for item in aplica_t5(anchor_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KTzbECt6l4Ds",
    "outputId": "9ec553de-a8ea-46d2-ff5a-3d7f4076074d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "Fim\n"
     ]
    }
   ],
   "source": [
    "tensor_list_target_test = [item.numpy() for item in aplica_t5(target_test)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VeaLT--rl-TL"
   },
   "source": [
    "Agora que já fizemos todo o processamento dos dados, podemos juntá-los novamente e iniciar o processo de aplicação de modelos! Vamos primeiro verificar como ficaria um dataset com os nossos dados pré processados!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "0v_7TMFomcm5"
   },
   "outputs": [],
   "source": [
    "df_dict_train = {'anchor': tensor_list_anchor_train, 'target': tensor_list_target_train, 'context': context_train} \n",
    "df_train_pre_process = pd.DataFrame(df_dict_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "SKKzNq12383w"
   },
   "outputs": [],
   "source": [
    "df_dict_test = {'anchor': tensor_list_anchor_test, 'target': tensor_list_target_test, 'context': context_test} \n",
    "df_test_pre_process = pd.DataFrame(df_dict_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "pC-C4WBAqaQ2",
    "outputId": "ade28920-1461-4628-f085-3fddb23be476"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anchor</th>\n",
       "      <th>target</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-0.0041455897, 0.008629801, -0.0015985373, 0....</td>\n",
       "      <td>[-0.005752636, 0.014921856, -0.026139496, 0.02...</td>\n",
       "      <td>H04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-0.017522456, -0.009105446, 0.011088807, -0.0...</td>\n",
       "      <td>[-0.026981762, -0.0037352955, 0.0013800176, 0....</td>\n",
       "      <td>G02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-0.004556078, 0.00073000207, 0.011585892, 0.0...</td>\n",
       "      <td>[-0.005817436, 0.015859123, 0.03723751, 0.0361...</td>\n",
       "      <td>H04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-0.0024025522, 0.0023999435, 1.757596e-05, 0....</td>\n",
       "      <td>[-0.0044816523, -0.0026907905, 0.00043605364, ...</td>\n",
       "      <td>H04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-0.029380755, 0.0067394325, 0.02307278, 0.027...</td>\n",
       "      <td>[-0.01638283, -0.002532923, 0.01957229, 0.0393...</td>\n",
       "      <td>H04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[-0.026818106, -0.04342738, 0.012493371, 0.014...</td>\n",
       "      <td>[-0.028035538, -0.025664665, 0.018312432, 0.02...</td>\n",
       "      <td>H04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[-0.013730726, 0.043138575, -0.019898336, 0.02...</td>\n",
       "      <td>[-0.024303623, -0.0037116008, -0.022282429, -0...</td>\n",
       "      <td>H04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[-0.007624526, 0.049074385, 0.009555302, 0.007...</td>\n",
       "      <td>[3.232523e-05, 0.030661648, 0.0046217744, 0.00...</td>\n",
       "      <td>G02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[-0.021826955, 0.04106771, 0.020565022, 0.0339...</td>\n",
       "      <td>[-0.021145925, 0.042495552, 0.006259948, 0.044...</td>\n",
       "      <td>H04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.0017017433, 0.023156075, -0.0060733515, 0.0...</td>\n",
       "      <td>[0.001962241, 0.016862769, 0.0026229185, 0.027...</td>\n",
       "      <td>H04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              anchor  \\\n",
       "0  [-0.0041455897, 0.008629801, -0.0015985373, 0....   \n",
       "1  [-0.017522456, -0.009105446, 0.011088807, -0.0...   \n",
       "2  [-0.004556078, 0.00073000207, 0.011585892, 0.0...   \n",
       "3  [-0.0024025522, 0.0023999435, 1.757596e-05, 0....   \n",
       "4  [-0.029380755, 0.0067394325, 0.02307278, 0.027...   \n",
       "5  [-0.026818106, -0.04342738, 0.012493371, 0.014...   \n",
       "6  [-0.013730726, 0.043138575, -0.019898336, 0.02...   \n",
       "7  [-0.007624526, 0.049074385, 0.009555302, 0.007...   \n",
       "8  [-0.021826955, 0.04106771, 0.020565022, 0.0339...   \n",
       "9  [0.0017017433, 0.023156075, -0.0060733515, 0.0...   \n",
       "\n",
       "                                              target context  \n",
       "0  [-0.005752636, 0.014921856, -0.026139496, 0.02...     H04  \n",
       "1  [-0.026981762, -0.0037352955, 0.0013800176, 0....     G02  \n",
       "2  [-0.005817436, 0.015859123, 0.03723751, 0.0361...     H04  \n",
       "3  [-0.0044816523, -0.0026907905, 0.00043605364, ...     H04  \n",
       "4  [-0.01638283, -0.002532923, 0.01957229, 0.0393...     H04  \n",
       "5  [-0.028035538, -0.025664665, 0.018312432, 0.02...     H04  \n",
       "6  [-0.024303623, -0.0037116008, -0.022282429, -0...     H04  \n",
       "7  [3.232523e-05, 0.030661648, 0.0046217744, 0.00...     G02  \n",
       "8  [-0.021145925, 0.042495552, 0.006259948, 0.044...     H04  \n",
       "9  [0.001962241, 0.016862769, 0.0026229185, 0.027...     H04  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_pre_process.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DW6MmMp9US4Q"
   },
   "source": [
    "Opa! Foi esquecido de realizar um encoder do contexto, isso porque precisamos traduzir para os modelos o que o H04 e G02 significam. Além disso, precisaremos concatenar a âncora e o target. Podemos criar uma pipeline para o processamento da coluna de context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "kDv2Ckmmt5w7"
   },
   "outputs": [],
   "source": [
    "enc = OrdinalEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "AgWCQPV_twC1"
   },
   "outputs": [],
   "source": [
    "cat_pipeline = Pipeline([\n",
    "    ('cat_encoder', enc),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "qOJPPmHUt9js"
   },
   "outputs": [],
   "source": [
    "cat_attribs = ['context']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXBLK3bnU26P"
   },
   "source": [
    "Apesar de só precisarmos processar uma coluna, podemos criar uma pipeline completa para em caso de futuras interações precisarmos de mais processos de mudança dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "kglVgVmIuD1k"
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "    ('cat', cat_pipeline, cat_attribs),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "EvoD9UQ0uTJW"
   },
   "outputs": [],
   "source": [
    "train_prepared = full_pipeline.fit_transform(df_train_pre_process)\n",
    "test_prepared = full_pipeline.fit_transform(df_test_pre_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZwFAyMONUm6w"
   },
   "source": [
    "Agora que temos todas as colunas processadas, podemos concatená-las para iniciar a aplicação do modelo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "ZdlPbzfQwWVX"
   },
   "outputs": [],
   "source": [
    "x_train = np.concatenate((tensor_list_anchor_train, tensor_list_target_train, train_prepared), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "P2qR9T8g4zN_"
   },
   "outputs": [],
   "source": [
    "x_test = np.concatenate((tensor_list_anchor_test, tensor_list_target_test, test_prepared), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4b6dac57"
   },
   "source": [
    "# Encontrando o melhor modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3aa2b308"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c8d9c47b"
   },
   "source": [
    "# Linear Regression\n",
    "A regressão linear é uma análise utilizada para prever o valor de uma varíavel (chamada de variável dependente) com base no valor de outra variável (variável independente). Essa forma de análise estima os coeficientes da equação linear, envolvendo uma ou mais variáveis independentes que melhor predizem o valor da variável dependente. O que a regressão linear faz é ajustar uma linha reta ou superfície que minimiza as discrepâncias entre os valores de saída previstos e reais, e geralmente as calculadoras de regressão linear usam o método de “mínimos quadrados” para descobrir a linha de melhor ajuste. Graças a essa simplicidade e facilidade na interpretação dos dados, esse é um método muito utilizado em diversas áreas, como biologia, ciências sociais, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UENwRaAsz368"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9c4944b6",
    "outputId": "54204766-401b-4570-addf-428eda8d6fe3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_linear_regression = LinearRegression()\n",
    "clf_linear_regression.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ojz4gOZPVGw2"
   },
   "source": [
    "Vamos verificar o resultado do modelo com um cross validation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-9mKx-Eez7z0",
    "outputId": "6b00ea65-d430-474b-dbc2-06075527b584"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.25737763e+09, -3.12316664e-01, -1.96110045e+09])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_linear_regression = cross_val_score(\n",
    "    clf_linear_regression,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    cv=3,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "scores_linear_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xtFv0NPJkIrn"
   },
   "source": [
    "O resultado do cross validation sai negativo por uma questão de interpretação. Se quisermos enxergar da maneira correta, é preciso inverter o sinal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "09761b93"
   },
   "source": [
    "# Bayesian Ridge\n",
    "Agora explicando um pouco sobre o método Bayesian Ridge, o objetivo dessa regressão é de determinar a probabilidade anterior para os parâmetros do modelo, em vez de identificar o \"melhor\" valor desses parâmetros. Ou seja, ele permite de um mecânismo atuar mesmo quando os dados são insuficientes ou mal distribuídos ao realizar uma regressão linear usando distruidores de probabilidade ao invés de pontos estimados. \n",
    "\n",
    "Para encontrarmos os valores probabilísticos, utilizamos a seguinte fórmula:    \n",
    "$ p(y⏐X,w,α)=N(y⏐Xw,α) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3cd4a9be",
    "outputId": "060d83b2-4f9e-45f8-b70d-4b6a0167dea9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BayesianRidge()"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_bayesian_ridge = BayesianRidge()\n",
    "clf_bayesian_ridge.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fW6fzpM28J8Y",
    "outputId": "2a8be7e8-20c6-46dd-cbca-8d1735586a11"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.22917419, -0.23166696, -0.23836405])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_bayesian_ridge = cross_val_score(\n",
    "    clf_bayesian_ridge,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    cv=3,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "scores_bayesian_ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d3839f76"
   },
   "source": [
    "# RandomForest\n",
    "Como o próprio nome diz, essa regressão cria árvores de decisão (decision trees) que são definidas pelas suas variáveis aleatórias e ao final formam uma floresta aleatória. Para entendermos esse método, primeiro precisamos entender o que seriam as decision trees. Basicamente o algoritmo criará uma estrutura similar a um fluxograma, com “nós” onde uma condição é verificada, e, se atendida, o fluxo segue por um ramo, caso contrário, por outro, sempre levando ao próximo nó, até a finalização da árvore.\n",
    "\n",
    "Agora já familizarizados com os conceitos, você deve estar se perguntando como ele funciona, pois bem, primeiro temos as seleções das amostras, as quais também são realizadas de forma aleatória ao coletar aleatoriamente dos dados de treinamento. Nesta etapa é utilizado o bootstrap, que é um método de reamostragem onde as amostras selecionadas podem ser repetidas na seleção. Com esta primeira seleção de amostras será construída a primeira árvore de decisão. Em seguida, o algoritmo de entropia ou o índice Gini, escolherá a melhor variável para compor o nó raiz, variando de acordo com o método utilizado. Vale lembrar que esse cálculo não acontece na totalidade dos dados de treinamento, e sim só em uma parcela deles. Para escolha da variável do próximo nó, novamente serão escolhidas duas (ou mais) variáveis, excluindo as já selecionadas anteriormente, e o processo de escolha se repetirá.\n",
    "\n",
    "Por fim, vale ressaltar que esse não é o melhor método para construção de uma árvore de decisão, já que o algoritmo pode escolher as duas piores variáveis em sua primeira seleção. Entretanto, essa é uma prática também poderosa já que, por ser aleatória, acaba evitando o overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bcc12fae",
    "outputId": "a96513da-8589-4974-b972-1af91a2d6bf3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-33-9172d54e5e98>:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  clf_random_forest.fit(x_train,y_train)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_depth=2)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_random_forest = RandomForestRegressor(max_depth=2)\n",
    "clf_random_forest.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CYT4Owvh8-5c",
    "outputId": "2557f164-3bcc-4d7f-b79c-48ff8f344229"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.24681879, -0.25182797, -0.25814291])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_random_forest = cross_val_score(\n",
    "    clf_random_forest,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    cv=3,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "scores_random_forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3w399N1rHfoD"
   },
   "source": [
    "# Kernel Ridge\n",
    "\n",
    "Os modelos de regressão de Kernel Ridge são modelos de regressão não paramétricos capazes de modelar relacionamentos lineares e não lineares entre variáveis preditoras e resultados. Entretanto, seus resultados podem ser muito sensíveis às escolhas de hiperparâmetros passados. Assim, a regressão funciona a partir da combinação da regressão de Ridge (mínimos quadrados lineares com regularização de norma l2) com o truque do kernel. Dessa forma, ele aprende uma função linear no espaço induzido pelo respectivo kernel e pelos dados. Vale lembrar que para kernels não lineares isso corresponde a uma função não linear no espaço original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hDzZ7UQWIUwI"
   },
   "outputs": [],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LWR84U7mIWxF",
    "outputId": "4795b15f-404b-43ac-a9e5-678eb353de79"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KernelRidge()"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_kernel_ridge = KernelRidge()\n",
    "clf_kernel_ridge.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6fmbZM-BJ_hS",
    "outputId": "733b0abd-29ba-4f1e-dcbd-ef68d176f9a8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.2293451 , -0.2317493 , -0.23842658])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_kernel_ridge = cross_val_score(\n",
    "    clf_kernel_ridge,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    cv=3,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "scores_kernel_ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yyhKzGvhKhvU"
   },
   "source": [
    "# ElasticNet\n",
    "\n",
    "O ElasticNet é um método de regressão regularizada que combina linearmente as penalidades L1 e L2 dos métodos Lasso e Ridge, ao aprender com suas deficiências o que melhorar na regularização de modelos estatísticos. Isso ocorre, pois a regressão melhora as limitações do Lasso, ou seja, onde o Lasso pega algumas amostras para dados de alta dimensão. O procedimento de ElasticNet prevê a inclusão de “n” número de variáveis até a saturação. Se as variáveis são grupos altamente correlacionados, Lasso tende a escolher uma variável de tais grupos e ignorar o resto completamente. Dessa forma, essa técnica é mais apropriada quando os dados dimensionais são maiores que o número de amostras usadas. Agrupamentos e seleção de variáveis são os principais papéis da técnica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iQPuV3n7Kl2K"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GazdLMsxKtBK",
    "outputId": "eb083fff-9380-4c1b-c8ca-d4865b765a10"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElasticNet()"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_elastic_net = ElasticNet()\n",
    "clf_elastic_net.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p-CSObUyLJ2a",
    "outputId": "072d7e59-7300-42b3-bb46-136b463a815d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.2603032 , -0.26550769, -0.2706656 ])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_elastic_net = cross_val_score(\n",
    "    clf_elastic_net,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    cv=3,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "scores_elastic_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-xLtnsmrDgZQ"
   },
   "source": [
    "# SVM\n",
    "\n",
    "SVM, também conhecido como Support Vector Machine, é um algoritmo que encontra um hiperplano em um espaço N-dimensional (N — o número de recursos) que classifica distintamente os pontos de dados. Primeiramente é necessário encontrar um hiperplano que realiza a distinção para criar duas classes de pontos. Nosso objetivo ao fazer esse agrupamento é de encontrar um plano que tenha a margem máxima, ou seja, a distância máxima entre os pontos de dados de ambas as classes. Ao maximizar a distância da margem fornecemos algum reforço para que pontos de dados futuros possam ser classificados com mais confiança.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/720/0*ecA4Ls8kBYSM5nza.webp\" alt=\"Alternative text\" />\n",
    "\n",
    "Os vetores de suporte dessa imagens são pontos de dados que estão mais próximos do hiperplano e que influenciam na posição e na orientação do hiperplano. Usando esses vetores de suporte, maximizamos a margem do classificador. Se excluíssemos esses vetores de suporte, alterará a posição do hiperplano e não conseguiríamos construir nosso SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mi9eWSzxDrL6"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xwSXbMQrDxzy",
    "outputId": "a3c88cf9-4337-4362-899d-7d4ef0e24969"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVR()"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_SVR = SVR()\n",
    "clf_SVR.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D6FipV3PEX_9",
    "outputId": "a612a7d5-e6e3-414f-a6eb-cbb7b30dc11d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.22080178, -0.22301181, -0.23011978])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_SVR = cross_val_score(\n",
    "    clf_SVR,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    cv=3,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "scores_SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 494
    },
    "id": "DOzCSLDpGEr6",
    "outputId": "9bc404b1-b0b5-4aa1-8148-5304c9f2a3c4"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame({\n",
    "    'Bayesian Ridge': -scores_bayesian_ridge,\n",
    "    'Random Forest': -scores_random_forest,\n",
    "    'Linear Reg' : -scores_linear_regression,\n",
    "    'SVM': -scores_SVR,\n",
    "    'Kernel Ridge': -scores_kernel_ridge,\n",
    "    'Elastic Net': -scores_elastic_net\n",
    "}).plot.box(\n",
    "    xlabel='Regressors',\n",
    "    ylabel=r'Root mean squared error',\n",
    "    figsize=(10, 8),\n",
    ");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S8sRhcBSGsFd"
   },
   "source": [
    "Vamos retirar o linear reg porque no plot acima podemos perceber que ele realmente foi muito ruim e isso está atrapalhando a visualização dos demais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "id": "K1xdkG2pG2Tv",
    "outputId": "789a2753-1a29-480e-c629-703fadf09f7b"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame({\n",
    "    'Bayesian Ridge': -scores_bayesian_ridge,\n",
    "    'Random Forest': -scores_random_forest,\n",
    "    'SVM': -scores_SVR,\n",
    "    'Kernel Ridge': -scores_kernel_ridge,\n",
    "    'Elastic Net': -scores_elastic_net\n",
    "}).plot.box(\n",
    "    xlabel='Regressors',\n",
    "    ylabel=r'Root mean squared error',\n",
    "    figsize=(10, 8),\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "983cceda"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TG_wpWkmuZFr"
   },
   "source": [
    "O gráfico deixa claro que os três melhores modelos foram a SVM, o Bayesian Ridge e o Kernel Ridge, e que entre o Kernel Ridge e o Bayesian Ridge não houveram grandes diferenças. Por isso, vamos tentar melhorar os resultados da SVM e do Bayesian Ridge através da hiperparametrização."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KeHXq1HwrBa1"
   },
   "source": [
    "# Hiperparametrizando os melhores modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lWUHPL_dul1l"
   },
   "outputs": [],
   "source": [
    "pipe_bayesian_ridge = Pipeline([\n",
    "    ('clf', clf_bayesian_ridge)\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'clf__alpha_1': [10, 100, 1000], \n",
    "    'clf__alpha_2': [1e-14, 1e-11, 1e-13], \n",
    "    'clf__lambda_1': [1e-13, 1e-12, 1e-14], \n",
    "    'clf__lambda_2': [1e-2,1e-1, 1, 10] \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6vAaBZirIaOL"
   },
   "source": [
    "Na célula acima estão valores especificos porque fizemos varias vezes o grindsearch, contudo, colocando varios valores de uma vez fazia com que o modelo rodasse por horas e horas, tornando inviável. Por isso colocavamos diversos intervalos até aproximar de um melhor intervalo como está acima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dIbxMJNPuxP6",
    "outputId": "c938b45d-ef1a-4936-9a11-6ab50a6a417a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 108 candidates, totalling 216 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "clf_br = GridSearchCV(pipe_bayesian_ridge, params, cv=2, scoring='neg_root_mean_squared_error', return_train_score=True, n_jobs=-1, verbose=3)\n",
    "clf_br = clf_br.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sxWHAPbAvR3b",
    "outputId": "f2b9d1ba-c351-407f-9785-d00f1fc61293"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__alpha_1': 10,\n",
       " 'clf__alpha_2': 1e-11,\n",
       " 'clf__lambda_1': 1e-12,\n",
       " 'clf__lambda_2': 1}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_br.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p4-CPlFNA7aw"
   },
   "outputs": [],
   "source": [
    "y_pred = clf_br.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SoyCHNuzxexT",
    "outputId": "064f1efd-db87-4d5d-9c8c-275ab9274ba9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bayesian Ridge [conjunto de treino]: RMSE = 0.21\n"
     ]
    }
   ],
   "source": [
    "clf_bayesian_ridge_mse_train = mean_squared_error(y_train, y_pred)\n",
    "clf_bayesian_ridge_rmse_train = np.sqrt(clf_bayesian_ridge_mse_train)\n",
    "print('Bayesian Ridge [conjunto de treino]: RMSE = {:.2f}'.format(clf_bayesian_ridge_rmse_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y4tymGwt2X5z",
    "outputId": "3955b721-d606-49f1-b1a4-c1fafd529528"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.22908947, -0.23132428, -0.23819551])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_Bayesian_ridge_hiper = cross_val_score(\n",
    "    clf_br,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    cv=3,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "scores_Bayesian_ridge_hiper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0PAT7DhnJENk"
   },
   "source": [
    "Vamos tentar fazer também a hiperparametrização para a SVM, já que foi também um dos melhores modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jBJMJpNaJKme"
   },
   "outputs": [],
   "source": [
    "pipe_SVM = Pipeline([\n",
    "    ('clf', clf_SVR)\n",
    "])\n",
    "\n",
    "params_svm = {\n",
    "    'clf__kernel': ['poly'], #fizemos grindsearchs antes e o melhor foi poly, entao reduzimos as opções para aumentar dos demais\n",
    "    'clf__degree': [8, 9, 10, 11, 12, 18], #os valores estao altos pois ja percebemos que sempre selecionava os altos\n",
    "    'clf__coef0': [0.0, 1.0],\n",
    "    'clf__C': [10.0, 5.0, 100.0, 200.0],\n",
    "    'clf__epsilon': [0.1, 0.5, 0.7]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "57VmE5OBJ3JB",
    "outputId": "6b35727b-be4e-4601-b8de-77693c402f3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 144 candidates, totalling 288 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "clf_svm = GridSearchCV(pipe_SVM, params_svm, cv=2, scoring='neg_root_mean_squared_error', return_train_score=True, n_jobs=-1, verbose=3)\n",
    "clf_svm = clf_svm.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o75KsbjXJ7w2",
    "outputId": "fc6d55d1-0e89-4785-9076-80827584b5cd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__C': 100.0,\n",
       " 'clf__coef0': 0.0,\n",
       " 'clf__degree': 12,\n",
       " 'clf__epsilon': 0.1,\n",
       " 'clf__kernel': 'poly'}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_svm.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BAJE_TgCKEmV"
   },
   "outputs": [],
   "source": [
    "y_pred = clf_svm.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "paost24bLB9Y",
    "outputId": "a1aec4d9-c52c-4af6-e724-d469619a1071"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR [conjunto de treino]: RMSE = 0.09\n"
     ]
    }
   ],
   "source": [
    "clf_SVR_mse_train = mean_squared_error(y_train, y_pred)\n",
    "clf_SVR_rmse_train = np.sqrt(clf_SVR_mse_train)\n",
    "print('SVR [conjunto de treino]: RMSE = {:.2f}'.format(clf_SVR_rmse_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kyF78MffM41J"
   },
   "source": [
    "Opa! Aparentemente esse foi o nosso melhor resultado até agora! Parece que realmente a hiperparametrização funcionou! =) \n",
    "\n",
    "Vamos verificar no teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f8seKYsbM_ql",
    "outputId": "35f86b2a-552e-4251-eed1-d9eae263fb29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR [conjunto de teste]: RMSE = 0.19\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf_svm.predict(x_test)\n",
    "clf_SVR_mse_test = mean_squared_error(y_test, y_pred)\n",
    "clf_SVR_rmse_test = np.sqrt(clf_SVR_mse_test)\n",
    "print('SVR [conjunto de teste]: RMSE = {:.2f}'.format(clf_SVR_rmse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MiNXLZzsTRLQ"
   },
   "source": [
    "O resultado não piorou drasticamente. A queda é plausível, não necessariamente significa um overfitting. Além disso se manteve como o melhor resultado até o momento dentre todos os modelos testados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G5Ij6Qb6yHUY"
   },
   "source": [
    "Vamos testar um crossvalidation dos modelos ajustados para verificar o novo desempenho generalizando o dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-fLTnc2voZGq",
    "outputId": "f61c9ff5-92f0-4057-a5fa-115f04e137cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.20598192, -0.20803719, -0.21436914])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_SVR_hiper = cross_val_score(\n",
    "    clf_svm,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    cv=3,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "scores_SVR_hiper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sJFWT8Te4sKI"
   },
   "source": [
    "Vamos verificar os resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "id": "zJZJAcKE4uX_",
    "outputId": "664af97a-2b6f-4a38-df8f-4d3eec5d3a5d"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame({\n",
    "    'Bayesian Ridge ajustado': -scores_Bayesian_ridge_hiper,\n",
    "    'SVM ajustado': -scores_SVR_hiper    \n",
    "}).plot.box(\n",
    "    xlabel='Regressors',\n",
    "    ylabel=r'Root mean squared error',\n",
    "    figsize=(10, 8),\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3BXMgkQWyT2c"
   },
   "source": [
    "O modelo bayesiano praticamente não teve melhoras, mas o SVM teve uma melhora em relação como estava antes, fazendo dele sem dúvida o melhor até o momento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0f985e67"
   },
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81b10565"
   },
   "source": [
    "## Redes Neurais\n",
    "\n",
    "Agora vamos tentar uma nova estratégia, vamos fazer o uso das redes neurais para entender se elas produzem um efeito melhor do que os modelos de Machine Learning previamente utilizados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d753b7bf"
   },
   "source": [
    "#### Rede Neural | Dense layers\n",
    "\n",
    "vamos tentar implementar um modelo mais simples de Redes Neurais, um só com camadas Densas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b60d6838",
    "outputId": "df973681-3307-4dbe-d2a7-f37def467dbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 500)               769000    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 50)                25050     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 255       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 794,305\n",
      "Trainable params: 794,305\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Dense = keras.models.Sequential([\n",
    "    keras.layers.InputLayer(x_train.shape[1]),\n",
    "    keras.layers.Dense(500),\n",
    "    keras.layers.Dense(50),\n",
    "    keras.layers.Dense(5)\n",
    "])\n",
    "Dense.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "99a8d114",
    "outputId": "4f996396-ad58-48fd-8f69-f8ab28fa2c60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5/5 [==============================] - 2s 136ms/step - loss: 2.8509 - val_loss: 2.1586\n",
      "Epoch 2/20\n",
      "5/5 [==============================] - 0s 54ms/step - loss: 1.0213 - val_loss: 0.9934\n",
      "Epoch 3/20\n",
      "5/5 [==============================] - 0s 55ms/step - loss: 0.4633 - val_loss: 0.2407\n",
      "Epoch 4/20\n",
      "5/5 [==============================] - 0s 55ms/step - loss: 0.3163 - val_loss: 0.2308\n",
      "Epoch 5/20\n",
      "5/5 [==============================] - 0s 50ms/step - loss: 0.1395 - val_loss: 0.1080\n",
      "Epoch 6/20\n",
      "5/5 [==============================] - 0s 49ms/step - loss: 0.1160 - val_loss: 0.1126\n",
      "Epoch 7/20\n",
      "5/5 [==============================] - 0s 56ms/step - loss: 0.0922 - val_loss: 0.0639\n",
      "Epoch 8/20\n",
      "5/5 [==============================] - 0s 58ms/step - loss: 0.0759 - val_loss: 0.0684\n",
      "Epoch 9/20\n",
      "5/5 [==============================] - 0s 55ms/step - loss: 0.0794 - val_loss: 0.0596\n",
      "Epoch 10/20\n",
      "5/5 [==============================] - 0s 54ms/step - loss: 0.0660 - val_loss: 0.0585\n",
      "Epoch 11/20\n",
      "5/5 [==============================] - 0s 66ms/step - loss: 0.0649 - val_loss: 0.0615\n",
      "Epoch 12/20\n",
      "5/5 [==============================] - 0s 68ms/step - loss: 0.0642 - val_loss: 0.0571\n",
      "Epoch 13/20\n",
      "5/5 [==============================] - 0s 57ms/step - loss: 0.0607 - val_loss: 0.0550\n",
      "Epoch 14/20\n",
      "5/5 [==============================] - 0s 57ms/step - loss: 0.0615 - val_loss: 0.0546\n",
      "Epoch 15/20\n",
      "5/5 [==============================] - 0s 64ms/step - loss: 0.0600 - val_loss: 0.0547\n",
      "Epoch 16/20\n",
      "5/5 [==============================] - 0s 60ms/step - loss: 0.0589 - val_loss: 0.0548\n",
      "Epoch 17/20\n",
      "5/5 [==============================] - 0s 56ms/step - loss: 0.0583 - val_loss: 0.0536\n",
      "Epoch 18/20\n",
      "5/5 [==============================] - 0s 55ms/step - loss: 0.0578 - val_loss: 0.0533\n",
      "Epoch 19/20\n",
      "5/5 [==============================] - 0s 53ms/step - loss: 0.0571 - val_loss: 0.0524\n",
      "Epoch 20/20\n",
      "5/5 [==============================] - 0s 60ms/step - loss: 0.0565 - val_loss: 0.0527\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21cd6097640>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=0.005)\n",
    "Dense.compile(loss=\"mse\", optimizer=optimizer)\n",
    "Dense.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=20,\n",
    "    batch_size=512,\n",
    "    validation_data=(x_test, y_test),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ac2d123c",
    "outputId": "0fab8ab0-c4b7-468e-da41-76ff2a7f8423"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/77 [==============================] - 0s 5ms/step - loss: 0.0568\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.0527\n",
      "Rede Neural: Dense [conjunto de train]: RMSE = 0.2383128833097371\n",
      "Rede Neural: Dense [conjunto de teste]: RMSE = 0.22960037765449026\n"
     ]
    }
   ],
   "source": [
    "Dense_loss_train= Dense.evaluate(x_train,y_train)\n",
    "Dense_loss_test= Dense.evaluate(x_test,y_test)\n",
    "print(f'Rede Neural: Dense [conjunto de train]: RMSE = {Dense_loss_train**(1/2)}')\n",
    "print(f'Rede Neural: Dense [conjunto de teste]: RMSE = {Dense_loss_test**(1/2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1522367f"
   },
   "source": [
    "### Novo Approach\n",
    "\n",
    "  Ao invés de concatenar **anchor**, **target** e **context**, concatenamos **anchor** com **context** e **target** com **context**, e no final ao passar os dados por um modelos fazemos um produto vetorial entre eles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "b69cdd9f"
   },
   "outputs": [],
   "source": [
    "x_anchor_train, x_target_train, x_context_train = x_train[:,0:768], x_train[:,768:1536], x_train[:,1536:1536]\n",
    "x_anchor_test, x_target_test, x_context_test = x_test[:,0:768], x_test[:,768:1536], x_test[:,1536:1536]\n",
    "\n",
    "x_train_novo = [x_anchor_train, x_target_train, x_context_train]\n",
    "x_test_novo = [x_anchor_test, x_target_test, x_context_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dc1ed752"
   },
   "source": [
    "#### Rede Neural | Modelo do Novo **Approach**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fa2454c4",
    "outputId": "e87e553d-968a-4af5-e2c2-a8d0d7b7caf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 768)]        0           []                               \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 0)]          0           []                               \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 768)]        0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 768)          0           ['input_2[0][0]',                \n",
      "                                                                  'input_4[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 768)          0           ['input_3[0][0]',                \n",
      "                                                                  'input_4[0][0]']                \n",
      "                                                                                                  \n",
      " sequential_1 (Sequential)      (None, 5)            409805      ['concatenate[0][0]',            \n",
      "                                                                  'concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " unit_normalization (UnitNormal  (None, 5)           0           ['sequential_1[0][0]']           \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " unit_normalization_1 (UnitNorm  (None, 5)           0           ['sequential_1[1][0]']           \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " dot (Dot)                      (None, 1)            0           ['unit_normalization[0][0]',     \n",
      "                                                                  'unit_normalization_1[0][0]']   \n",
      "                                                                                                  \n",
      " softmax (Softmax)              (None, 1)            0           ['dot[0][0]']                    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 409,805\n",
      "Trainable params: 409,805\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input1 = keras.layers.Input(x_anchor_train.shape[1])\n",
    "input2 = keras.layers.Input(x_target_train.shape[1])\n",
    "context = keras.layers.Input(x_context_train.shape[1])\n",
    "c1 = keras.layers.Concatenate()([input1,context])\n",
    "c2 = keras.layers.Concatenate()([input2,context])\n",
    "modelzinho = keras.models.Sequential([\n",
    "    keras.layers.Dense(500),\n",
    "    keras.layers.Dense(50),\n",
    "    keras.layers.Dense(5)\n",
    "])\n",
    "x1 =modelzinho(c1)\n",
    "x2 = modelzinho(c2)\n",
    "n1 = keras.layers.UnitNormalization()(x1)\n",
    "n2 = keras.layers.UnitNormalization()(x2)\n",
    "p = keras.layers.Dot(axes=1)([n1,n2])\n",
    "s = keras.layers.Softmax()(p)\n",
    "big_model = keras.Model(inputs =[input1,input2,context]  , outputs=[s]  )\n",
    "big_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "6df3fbbe",
    "outputId": "9a34c4e9-f81a-44b6-ee53-0cd97d26c4f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "keras.utils.plot_model(big_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b7493047",
    "outputId": "35603b7f-5132-41a6-c8ef-f5c0b5dadf03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 103ms/step - loss: 0.4784 - val_loss: 0.4973\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 0.4784 - val_loss: 0.4973\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 52ms/step - loss: 0.4784 - val_loss: 0.4973\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 48ms/step - loss: 0.4784 - val_loss: 0.4973\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.4784 - val_loss: 0.4973\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.4784 - val_loss: 0.4973\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 50ms/step - loss: 0.4784 - val_loss: 0.4973\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 58ms/step - loss: 0.4784 - val_loss: 0.4973\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 53ms/step - loss: 0.4784 - val_loss: 0.4973\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 49ms/step - loss: 0.4784 - val_loss: 0.4973\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 49ms/step - loss: 0.4784 - val_loss: 0.4973\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 51ms/step - loss: 0.4784 - val_loss: 0.4973\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 48ms/step - loss: 0.4784 - val_loss: 0.4973\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 47ms/step - loss: 0.4784 - val_loss: 0.4973\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.4784 - val_loss: 0.4973\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 49ms/step - loss: 0.4784 - val_loss: 0.4973\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 51ms/step - loss: 0.4784 - val_loss: 0.4973\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 58ms/step - loss: 0.4784 - val_loss: 0.4973\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 62ms/step - loss: 0.4784 - val_loss: 0.4973\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 62ms/step - loss: 0.4784 - val_loss: 0.4973\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 47ms/step - loss: 0.4784 - val_loss: 0.4973\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.4784 - val_loss: 0.4973\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.4784 - val_loss: 0.4973\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 49ms/step - loss: 0.4784 - val_loss: 0.4973\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 50ms/step - loss: 0.4784 - val_loss: 0.4973\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 52ms/step - loss: 0.4784 - val_loss: 0.4973\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 50ms/step - loss: 0.4784 - val_loss: 0.4973\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 52ms/step - loss: 0.4784 - val_loss: 0.4973\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 49ms/step - loss: 0.4784 - val_loss: 0.4973\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 50ms/step - loss: 0.4784 - val_loss: 0.4973\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 50ms/step - loss: 0.4784 - val_loss: 0.4973\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 49ms/step - loss: 0.4784 - val_loss: 0.4973\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 61ms/step - loss: 0.4784 - val_loss: 0.4973\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 63ms/step - loss: 0.4784 - val_loss: 0.4973\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 60ms/step - loss: 0.4784 - val_loss: 0.4973\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 64ms/step - loss: 0.4784 - val_loss: 0.4973\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 56ms/step - loss: 0.4784 - val_loss: 0.4973\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 48ms/step - loss: 0.4784 - val_loss: 0.4973\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 48ms/step - loss: 0.4784 - val_loss: 0.4973\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 52ms/step - loss: 0.4784 - val_loss: 0.4973\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 55ms/step - loss: 0.4784 - val_loss: 0.4973\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 60ms/step - loss: 0.4784 - val_loss: 0.4973\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 59ms/step - loss: 0.4784 - val_loss: 0.4973\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 51ms/step - loss: 0.4784 - val_loss: 0.4973\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 49ms/step - loss: 0.4784 - val_loss: 0.4973\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 56ms/step - loss: 0.4784 - val_loss: 0.4973\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 53ms/step - loss: 0.4784 - val_loss: 0.4973\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 61ms/step - loss: 0.4784 - val_loss: 0.4973\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 51ms/step - loss: 0.4784 - val_loss: 0.4973\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 48ms/step - loss: 0.4784 - val_loss: 0.4973\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21cf5c15db0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=0.005)\n",
    "big_model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "big_model.fit(\n",
    "    x_train_novo,\n",
    "    y_train,\n",
    "    epochs=50,\n",
    "    batch_size=512,\n",
    "    validation_data=(x_test_novo, y_test),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "79cfee73",
    "outputId": "8c3df270-19f0-4cc6-d8d0-16d73907573d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/77 [==============================] - 0s 4ms/step - loss: 0.4784\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.4973\n",
      "Rede Neural: Big_model [conjunto de train]: RMSE = 0.6916702111471273\n",
      "Rede Neural: Big_model [conjunto de teste]: RMSE = 0.7052236602297173\n"
     ]
    }
   ],
   "source": [
    "big_model_loss_train= big_model.evaluate(x_train_novo,y_train)\n",
    "big_model_loss_test= big_model.evaluate(x_test_novo,y_test)\n",
    "print(f'Rede Neural: Big_model [conjunto de train]: RMSE = {big_model_loss_train**(1/2)}')\n",
    "print(f'Rede Neural: Big_model [conjunto de teste]: RMSE = {big_model_loss_test**(1/2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3d1e2fec"
   },
   "source": [
    "#### Rede Neural | Pivotando o modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b42542d0"
   },
   "source": [
    "Como podemos ver, o modelo de Rede Neural anterior não teve um **RMSE** muito bom quando comparando a primeira tentativa de rede neural. Fazer a separação do **target** e **anchor** e junto do contexto e depois juntart tudo com produto vetorial não pareceu uma boa ideia, nesse sentido, vamos voltar a primeira estratégia de utilizar o **target**, **anchor** e **context** todos concatenados e tentar melhorar o primeiro modelo de Rede Neural que tentamos\n",
    "\n",
    "**OBS**: Foi tentado utlizar outras camadas com **RNN** e **LSTM**, mas por motivos de recurso computacional, não deu para ser implemetado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0da7676a",
    "outputId": "fa424cd1-33a5-46fe-9533-b4760d8158c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_11 (InputLayer)       [(None, 1537)]            0         \n",
      "                                                                 \n",
      " sequential_8 (Sequential)   (None, 100)               14171767  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,171,767\n",
      "Trainable params: 14,161,767\n",
      "Non-trainable params: 10,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input = keras.layers.Input((x_train.shape[1]))\n",
    "NN = keras.models.Sequential([\n",
    "    keras.layers.Dropout(0.25),\n",
    "    keras.layers.Dense(5000),\n",
    "    keras.layers.Dropout(0.35),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(1000),\n",
    "    keras.layers.Dense(750),\n",
    "    keras.layers.Dense(500, activation=\"tanh\"),\n",
    "    keras.layers.Dropout(0.45),\n",
    "    keras.layers.Dense(430),\n",
    "    keras.layers.Dense(222),\n",
    "    keras.layers.Dense(100),\n",
    "    keras.layers.Dropout(0.45),\n",
    "    keras.layers.Dense(5),\n",
    "    keras.layers.GaussianNoise(stddev=1),\n",
    "    keras.layers.Dropout(0.50),\n",
    "    keras.layers.Dense(100),\n",
    "])\n",
    "X = NN(input)\n",
    "new_model = keras.Model(inputs=input, outputs=[X])\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "09d3c660",
    "outputId": "65f2114c-1ea5-4966-9a76-b4c19f2ec6a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5/5 [==============================] - 4s 673ms/step - loss: 0.1074 - val_loss: 0.0648\n",
      "Epoch 2/20\n",
      "5/5 [==============================] - 3s 583ms/step - loss: 0.0827 - val_loss: 0.0635\n",
      "Epoch 3/20\n",
      "5/5 [==============================] - 3s 577ms/step - loss: 0.0742 - val_loss: 0.0626\n",
      "Epoch 4/20\n",
      "5/5 [==============================] - 3s 680ms/step - loss: 0.0730 - val_loss: 0.0618\n",
      "Epoch 5/20\n",
      "5/5 [==============================] - 3s 607ms/step - loss: 0.0723 - val_loss: 0.0619\n",
      "Epoch 6/20\n",
      "5/5 [==============================] - 3s 580ms/step - loss: 0.0714 - val_loss: 0.0616\n",
      "Epoch 7/20\n",
      "5/5 [==============================] - 3s 548ms/step - loss: 0.0711 - val_loss: 0.0614\n",
      "Epoch 8/20\n",
      "5/5 [==============================] - 3s 557ms/step - loss: 0.0705 - val_loss: 0.0614\n",
      "Epoch 9/20\n",
      "5/5 [==============================] - 3s 545ms/step - loss: 0.0708 - val_loss: 0.0615\n",
      "Epoch 10/20\n",
      "5/5 [==============================] - 3s 572ms/step - loss: 0.0705 - val_loss: 0.0616\n",
      "Epoch 11/20\n",
      "5/5 [==============================] - 3s 540ms/step - loss: 0.0703 - val_loss: 0.0615\n",
      "Epoch 12/20\n",
      "5/5 [==============================] - 3s 555ms/step - loss: 0.0705 - val_loss: 0.0617\n",
      "Epoch 13/20\n",
      "5/5 [==============================] - 3s 552ms/step - loss: 0.0702 - val_loss: 0.0616\n",
      "Epoch 14/20\n",
      "5/5 [==============================] - 3s 550ms/step - loss: 0.0702 - val_loss: 0.0615\n",
      "Epoch 15/20\n",
      "5/5 [==============================] - 3s 541ms/step - loss: 0.0703 - val_loss: 0.0617\n",
      "Epoch 16/20\n",
      "5/5 [==============================] - 3s 544ms/step - loss: 0.0703 - val_loss: 0.0616\n",
      "Epoch 17/20\n",
      "5/5 [==============================] - 3s 550ms/step - loss: 0.0703 - val_loss: 0.0616\n",
      "Epoch 18/20\n",
      "5/5 [==============================] - 3s 549ms/step - loss: 0.0704 - val_loss: 0.0615\n",
      "Epoch 19/20\n",
      "5/5 [==============================] - 3s 543ms/step - loss: 0.0702 - val_loss: 0.0616\n",
      "Epoch 20/20\n",
      "5/5 [==============================] - 3s 571ms/step - loss: 0.0703 - val_loss: 0.0617\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21c9b981210>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=0.005)\n",
    "new_model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "new_model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=20,\n",
    "    batch_size=512,\n",
    "    validation_data=(x_test, y_test),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "29195555"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0700\n",
      "20/20 [==============================] - 1s 23ms/step - loss: 0.0617\n",
      "Rede Neural: New_model [conjunto de train]: RMSE = 0.26453402815755256\n",
      "Rede Neural: New_model [conjunto de teste]: RMSE = 0.2484876366174375\n"
     ]
    }
   ],
   "source": [
    "new_model_loss_train= new_model.evaluate(x_train,y_train)\n",
    "new_model_loss_test= new_model.evaluate(x_test,y_test)\n",
    "print(f'Rede Neural: New_model [conjunto de train]: RMSE = {new_model_loss_train**(1/2)}')\n",
    "print(f'Rede Neural: New_model [conjunto de teste]: RMSE = {new_model_loss_test**(1/2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusão:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O modelo que melhor explicou o problema foi o SVM quando hiperparametrizado. A abordagem de redes não trouxe melhoria nos resultados para o problema a ser estudado.\n",
    "\n",
    "Infelizmente não foi possível enviar o modelo para o kaggle e assim obter uma classificação no desafio pois foi preciso recortar os dados devido ao desafio computacional. Não apenas quando rodando a parte de redes, mas também quando realizando os cross validations dos modelos, o próprio colab crashava. \n",
    "\n",
    "Também tivemos problemas com o monstrão devido a alta procura e requisição de outras matérias por ele, então nossa máquina sempre precisava ser desligada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "yT6ZlOV-RpH-",
    "4b6dac57",
    "c8d9c47b",
    "09761b93",
    "d3839f76",
    "3w399N1rHfoD",
    "yyhKzGvhKhvU"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
